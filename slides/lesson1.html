<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lesson 1 — From Regression to Deep Learning</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/theme/white.css" id="theme">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/highlight/monokai.css">
    <style>
      .reveal section pre code { font-size: 0.9em; line-height: 1.35; }
      .small { font-size: 0.8em; }
      .img-row img { margin: 0.25rem 0.5rem; border: 1px solid #eee; }
      .graphviz svg { max-width: 80%; height: auto; }
      .activation-plot { margin: 0.3rem 0 0.5rem; }
      .activation-plot img { max-width: 45%; height: auto; }
    </style>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h2>Lesson 1: From Regression to Deep Learning</h2>
          <h3>Learning Objectives</h3>
          <ul>
            <li class="fragment">Why linear models fail on complex data</li>
            <li class="fragment">How layers + activations extend regression</li>
            <li class="fragment">Visualize decision boundaries forming</li>
            <li class="fragment">Forward pass intuition with minimal math</li>
          </ul>
          
        </section>

        <section>
          <h3>Why Regression Isn’t Enough</h3>
          <ul>
            <li>Recall logistic regression decision boundaries</li>
            <li>Curved datasets (circles, spirals) break linear separability</li>
            <li>Prompt: “How could we bend this decision line?”</li>
          </ul>
          <div class="img-row">
            <img src="assets/circles.svg" alt="Concentric circles" width="44%" />
            <img src="assets/spirals.svg" alt="Intertwined spirals" width="44%" />
          </div>
          
        </section>

        <section>
          <h3>Notation</h3>
          <ul>
            <li class="fragment">Input vector: \(\mathbf{x} \in \mathbb{R}^d\)</li>
            <li class="fragment">Weights: \(\mathbf{w} \in \mathbb{R}^d\), bias: \(b \in \mathbb{R}\)</li>
            <li class="fragment">Activation (nonlinearity): \(\sigma(\cdot)\) e.g., ReLU, sigmoid</li>
          </ul>
          <p class="small">Tip: hidden layers typically use ReLU; outputs use sigmoid for binary classification.</p>
          
        </section>

        <section>
          <h3>Single Neuron (Scalar Output)</h3>
          <p>Linear combination + nonlinearity:</p>
          <p style="font-size:1.2em">\[ \hat{y} = \sigma\big( \mathbf{w}^\top \mathbf{x} + b \big) \]</p>
          <p class="small">Without \(\sigma\), this is just linear regression/logistic logit.</p>
          <p class="small">Shapes: \(\mathbf{x}\in\mathbb{R}^d\), \(\mathbf{w}\in\mathbb{R}^d\), \(b\in\mathbb{R}\), \(\hat{y}\in\mathbb{R}\). Common \(\sigma\): ReLU, tanh, sigmoid.</p>
          
        </section>

        <section>
          <h3>Neuron + Bias Intuition</h3>
          <ul>
            <li class="fragment">Linear part: z = w · x + b</li>
            <li class="fragment">Activation: a = σ(z) adds nonlinearity</li>
            <li class="fragment">Bias b moves the decision threshold</li>
            <li class="fragment">Layers stack these simple units</li>
          </ul>
        </section>

        <section>
          <h3>Diagram: Single Neuron</h3>
          <pre><code class="language-dot" data-graphviz>
digraph neuron {
  rankdir=LR;
  node [shape=circle, fontsize=12];

  x1 [label="x₁"];
  x2 [label="x₂"];
  x3 [label="x₃"];
  h  [label="σ(w·x + b)"];

  x1 -> h;
  x2 -> h;
  x3 -> h;
}
          </code></pre>
          <p class="small">Multiple inputs are combined into a single neuron that applies σ to \(w \cdot x + b\).</p>
        </section>

        <section>
          <h3>ReLU Activation</h3>
          <p class="small">\(\mathrm{ReLU}(z) = \max(0, z)\) — default for hidden layers.</p>
          <div class="activation-plot">
            <img src="assets/relu_activation.png" alt="ReLU activation function" />
          </div>
          <ul>
            <li class="fragment">Zero for negative inputs, linear for positive</li>
            <li class="fragment">Keeps computation simple and fast</li>
            <li class="fragment">Works well as a default for hidden layers</li>
          </ul>
        </section>

        <section>
          <h3>Sigmoid Activation</h3>
          <p class="small">\(\sigma(z) = \frac{1}{1 + e^{-z}}\) — used for binary outputs.</p>
          <div class="activation-plot">
            <img src="assets/sigmoid_activation.png" alt="Sigmoid activation function" />
          </div>
          <ul>
            <li class="fragment">Maps any real number to (0, 1)</li>
            <li class="fragment">Interpretable as probability for binary class</li>
            <li class="fragment">Used on the final neuron in this lesson</li>
          </ul>
        </section>

        <section>
          <h3>Layer (Vector Form)</h3>
          <p>Compute multiple neurons at once:</p>
          <p style="font-size:1.2em">\[ \mathbf{a}^{(1)} = \sigma\big( \mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)} \big) \]</p>
          <p class="small">\(\mathbf{W}^{(1)} \in \mathbb{R}^{m\times d}\) maps input to \(m\) hidden units.</p>
          <p class="small">Shapes: \(\mathbf{b}^{(1)}\in\mathbb{R}^m\), \(\mathbf{a}^{(1)}\in\mathbb{R}^m\). \(\sigma\) applies elementwise.</p>
          
        </section>

        <section>
          <h3>Two-Layer Network</h3>
          <p>Compose layers to get flexible decision boundaries:</p>
          <p style="font-size:1.2em">\[ \hat{y} = \sigma^{(2)}\!\big( \mathbf{W}^{(2)} \, \sigma^{(1)}( \mathbf{W}^{(1)}\mathbf{x} + \mathbf{b}^{(1)} ) + \mathbf{b}^{(2)} \big) \]</p>
          <p class="small">Each layer: linear map + nonlinearity. Stacking learns features of features.</p>
          <p class="small">Shapes: hidden size \(m\), output size \(k\). \(\mathbf{W}^{(2)}\in\mathbb{R}^{k\times m}\), \(\mathbf{b}^{(2)}\in\mathbb{R}^k\), \(\hat{y}\in\mathbb{R}^k\). Typical in this lesson: \(\sigma^{(1)}=\)ReLU, \(\sigma^{(2)}=\)sigmoid.</p>
          
        </section>

        <section>
          <h3>Diagram: Two-Layer Network</h3>
          <pre><code class="language-dot" data-graphviz>
digraph two_layer {
  rankdir=LR;
  node [shape=circle, fontsize=12];

  subgraph cluster_input {
    label="Inputs";
    color="white";
    x1 [label="x₁"];
    x2 [label="x₂"];
  }

  subgraph cluster_hidden {
    label="Hidden layer (ReLU)";
    color="lightgray";
    h1 [label="h₁"];
    h2 [label="h₂"];
    h3 [label="h₃"];
  }

  subgraph cluster_output {
    label="Output";
    color="white";
    y [label="ŷ"];
  }

  x1 -> h1; x1 -> h2; x1 -> h3;
  x2 -> h1; x2 -> h2; x2 -> h3;
  h1 -> y;  h2 -> y;  h3 -> y;
}
          </code></pre>
          <p class="small">Inputs feed into hidden units, which then feed into the output neuron.</p>
          
        </section>

        <section>
          <h3>Forward Pass — Step by Step</h3>
          <ol>
            <li class="fragment">Compute each layer’s z = W·x + b</li>
            <li class="fragment">Apply activation a = σ(z)</li>
            <li class="fragment">Feed a into next layer</li>
            <li class="fragment">Final output → prediction</li>
          </ol>
          <p class="small">We’ll worry about learning (backprop) next lesson.</p>
          
        </section>

        <section>
          <h3>Key Themes</h3>
          <ul>
            <li class="fragment">Linear part learns weighted combinations; bias shifts thresholds</li>
            <li class="fragment">Nonlinearity lets boundaries bend (beyond any single line)</li>
            <li class="fragment">Depth composes simple units into complex patterns</li>
          </ul>
          
        </section>

        <section>
          <h3>Interactive Demo (TF Playground)</h3>
          <ol>
            <li>Select circular or spiral dataset</li>
            <li>No hidden layers → poor separation</li>
            <li>1 hidden layer (8) → improvement</li>
            <li>2 layers (8+8) → complex shapes classified</li>
            <li>Tune activations, learning rate, noise</li>
          </ol>
          <p class="small"><em>Discuss:</em> Why deeper → better patterns? What might each neuron learn?</p>
          <p class="small"><a href="https://playground.tensorflow.org" target="_blank">Open TensorFlow Playground</a></p>
          
        </section>

        <section>
          <h3>Mini Hands-On</h3>
          <p>Implement the idea in PyTorch on synthetic 2D points.</p>
          <pre><code class="language-python">import torch
import torch.nn as nn

model = nn.Sequential(
  nn.Linear(2, 8),
  nn.ReLU(),
  nn.Linear(8, 8),
  nn.ReLU(),
  nn.Linear(1, 1),
  nn.Sigmoid(),
)
# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
# loss_fn = nn.BCELoss()
# training loop ...
          </code></pre>
          <p class="small">Plot decision regions; relate to Playground behavior.</p>
          
        </section>

        <section>
          <h3>Code: Make a 2D Dataset</h3>
          <pre><code class="language-python"># Option A: use scikit-learn (concise)
from sklearn.datasets import make_circles
import numpy as np

X, y = make_circles(n_samples=1000, factor=0.5, noise=0.1, random_state=0)
X = X.astype('float32')
y = y.astype('float32')

# Option B: quick NumPy spiral (for the curious)
def make_spiral(n=500, noise=0.2):
    n2 = n//2
    t = np.linspace(0, 2*np.pi, n2)
    r = np.linspace(0.2, 1.0, n2)
    x1 = np.c_[r*np.cos(t), r*np.sin(t)] + noise*np.random.randn(n2,2)
    x2 = np.c_[-r*np.cos(t), -r*np.sin(t)] + noise*np.random.randn(n2,2)
    Xs = np.vstack([x1, x2]).astype('float32')
    ys = np.r_[np.zeros(n2), np.ones(n2)].astype('float32')
    return Xs, ys
          </code></pre>
          
        </section>

        <section>
          <h3>Code: Train and Evaluate (PyTorch)</h3>
          <pre><code class="language-python">import torch
import torch.nn as nn

X_t = torch.from_numpy(X)
y_t = torch.from_numpy(y).unsqueeze(1)

model = nn.Sequential(
    nn.Linear(2, 8),
    nn.ReLU(),
    nn.Linear(8, 8),
    nn.ReLU(),
    nn.Linear(8, 1),
    nn.Sigmoid(),
)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_fn = nn.BCELoss()

for epoch in range(20):
    optimizer.zero_grad()
    preds = model(X_t)
    loss = loss_fn(preds, y_t)
    loss.backward()
    optimizer.step()

print('Final training loss:', float(loss))
          </code></pre>
          <p class="small">Expect validation accuracy to improve over epochs; details next lesson.</p>
          
        </section>

        <section>
          <h3>Code: Plot Decision Regions</h3>
          <pre><code class="language-python">import numpy as np, matplotlib.pyplot as plt
import torch

xx, yy = np.meshgrid(np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 200),
                     np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 200))
grid = np.c_[xx.ravel(), yy.ravel()].astype('float32')
with torch.no_grad():
    grid_t = torch.from_numpy(grid)
    probs = model(grid_t).detach().numpy().reshape(xx.shape)

plt.figure(figsize=(5,4))
plt.contourf(xx, yy, probs, levels=20, cmap='RdBu', alpha=0.6)
plt.scatter(X[:,0], X[:,1], c=y, cmap='RdBu', edgecolor='k', s=12)
plt.title('Decision regions')
plt.show()
          </code></pre>
          <p class="small">Relate the learned boundary to the Playground visuals.</p>
          
        </section>

        <section>
          <h3>Wrap-Up</h3>
          <ul>
            <li>Neuron = regression + nonlinearity on top</li>
            <li>Layers stack neurons to learn features of features</li>
            <li>Depth + activations bend decision boundaries into complex shapes</li>
          </ul>
          <p class="small"><strong>Homework:</strong></p>
          <ul class="small">
            <li>In the spirals notebook, recreate one of the Playground experiments (spirals or circles).</li>
            <li>Try at least two different architectures.</li>
            <li>Find the deepest network (most layers) that uses the fewest units but still fits the data.</li>
            <li>Find the shallowest network (fewest layers) that uses the fewest units but still fits the data.</li>
          </ul>
          <p class="small">Next time: how networks actually learn these weights using loss functions and gradient descent.</p>
          
        </section>

      </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/viz.js@2.1.2/viz.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/viz.js@2.1.2/full.render.js"></script>
    <script>
      Reveal.initialize({
        hash: true,
        slideNumber: true,
        transition: 'slide',
        plugins: [RevealNotes, RevealHighlight]
      });

      (function () {
        if (typeof Viz === 'undefined') {
          return;
        }
        const viz = new Viz();
        const renderGraphs = () => {
          const blocks = document.querySelectorAll('code.language-dot[data-graphviz]');
          blocks.forEach((code) => {
            const dot = code.textContent;
            viz.renderSVGElement(dot)
              .then((svg) => {
                const container = document.createElement('div');
                container.className = 'graphviz';
                container.appendChild(svg);
                const pre = code.parentNode;
                if (pre && pre.parentNode) {
                  pre.parentNode.replaceChild(container, pre);
                }
              })
              .catch((err) => {
                console.error('Graphviz render error:', err);
              });
          });
        };
        if (document.readyState === 'loading') {
          document.addEventListener('DOMContentLoaded', renderGraphs);
        } else {
          renderGraphs();
        }
      })();
    </script>
  </body>
  </html>
