<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lesson 4 — Data Augmentation & CNN Architectures</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/theme/white.css" id="theme">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/highlight/monokai.css">
    <link rel="stylesheet" href="assets/slides-common.css">
    <style>
      /* Lesson 4 specific overrides */
      .code-small pre code { font-size: 0.8em; }
      .two-column { display: flex; gap: 2rem; align-items: flex-start; }
      .two-column > div { flex: 1; }
      .timeline ul { list-style: none; padding-left: 0; }
      .timeline li { margin: 0.25rem 0; }
    </style>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h2>Lesson 4: Data Augmentation & CNN Architectures</h2>
          <h3>Learning Objectives</h3>
          <ul>
            <li class="fragment">Understand why and when we use data augmentation for vision models</li>
            <li class="fragment">Recognize common augmentation operations and how to implement them in PyTorch</li>
            <li class="fragment">Build intuition for the evolution of CNN architectures from LeNet to ResNet</li>
            <li class="fragment">Connect CNN features to downstream tasks like object tracking</li>
          </ul>
        </section>

        <section>
          <h3>Recap: CNNs and Overfitting</h3>
          <ul>
            <li class="fragment">CNNs can have millions of parameters and easily memorize the training set</li>
            <li class="fragment">Overfitting symptoms: training loss \(\downarrow\) while validation loss \(\uparrow\) or plateaus early</li>
            <li class="fragment">We already saw regularization like Dropout and stabilization techniques like BatchNorm</li>
            <li class="fragment">In this lesson we add <strong>weight decay</strong> and <strong>data augmentation</strong> to our toolkit</li>
          </ul>
        </section>

        <section>
          <h3>Regularization with Weight Decay</h3>
          <p class="small">Weight decay (L2 regularization) discourages very large weights by adding a penalty to the loss.</p>
          <p class="small">\[
            L_{\text{total}} = L_{\text{data}} + \lambda \lVert W \rVert_2^2
          \]</p>
          <ul class="small">
            <li class="fragment">\(\lambda\) controls the strength of the penalty (e.g., \(10^{-4}\) or \(10^{-3}\))</li>
            <li class="fragment">Encourages simpler models and can reduce overfitting, especially with many parameters</li>
            <li class="fragment">In PyTorch, set <code>weight_decay</code> in the optimizer instead of manually modifying the loss</li>
          </ul>
          <pre><code class="language-python">import torch.optim as optim

optimizer = optim.Adam(model.parameters(),
                       lr=1e-3,
                       weight_decay=1e-4)  # L2 penalty</code></pre>
          <p class="small">Combine weight decay with Dropout, BatchNorm, and data augmentation for more robust CNNs.</p>
        </section>

        <section>
          <h3>Idea: Data Augmentation</h3>
          <p class="small">Data augmentation creates new training examples by applying label-preserving transformations.</p>
          <ul>
            <li class="fragment">Example: flipping, rotating, cropping, or color-jittering images</li>
            <li class="fragment">We do <em>not</em> change the label — a rotated cat is still a cat</li>
            <li class="fragment">Model learns to be robust to common variations (pose, lighting, small translations)</li>
            <li class="fragment">Acts like training on a much larger dataset without collecting more images</li>
          </ul>
        </section>

        <section>
          <h3>Types of Augmentation</h3>
          <div class="two-column small">
            <div>
              <strong>Geometric</strong>
              <ul>
                <li class="fragment">Random crop / resize</li>
                <li class="fragment">Horizontal / vertical flip</li>
                <li class="fragment">Small rotations (e.g., \(\pm 10^\circ\))</li>
                <li class="fragment">Random affine transforms (scale, shear)</li>
              </ul>
            </div>
            <div>
              <strong>Photometric</strong>
              <ul>
                <li class="fragment">Brightness / contrast changes</li>
                <li class="fragment">Color jitter (hue, saturation)</li>
                <li class="fragment">Gaussian noise or blur</li>
                <li class="fragment">Random grayscale</li>
              </ul>
            </div>
          </div>
          <ul class="small">
            <li class="fragment">Choose transforms that make sense for your data and task (e.g., avoid vertical flips for digits)</li>
          </ul>
        </section>

        <section>
          <h3>Data Augmentation Pipeline</h3>
          <pre><code class="language-dot" data-graphviz>
digraph aug_pipeline {
  rankdir=LR;
  node [fontsize=11, shape=box, style=rounded];

  raw   [label="Original image\n(train sample)"];
  aug   [label="Random transforms\n(flip, rotate, color jitter)"];
  batch [label="Augmented batch\n(x_batch, y_batch)"];
  model [label="CNN model\n(f(x; θ))"];

  raw -> aug -> batch -> model;
}
          </code></pre>
          <p class="small">Each epoch, the same image can look different → the model sees a stream of varied views.</p>
        </section>

        <section>
          <h3>PyTorch: Augmentation with <code>torchvision.transforms</code></h3>
          <pre><code class="language-python">import torchvision.transforms as T

train_transform = T.Compose([
    T.RandomHorizontalFlip(p=0.5),
    T.RandomRotation(degrees=10),
    T.ColorJitter(brightness=0.2, contrast=0.2),
    T.ToTensor(),
    T.Normalize((0.1307,), (0.3081,)),  # MNIST stats
])

test_transform = T.Compose([
    T.ToTensor(),
    T.Normalize((0.1307,), (0.3081,)),
])</code></pre>
          <p class="small">Apply <code>train_transform</code> only to training data; keep validation/test transforms deterministic.</p>
        </section>

        <section>
          <h3>Using Transforms in a Dataset</h3>
          <pre><code class="language-python">from torchvision import datasets
from torch.utils.data import DataLoader

train_ds = datasets.MNIST(
    root="data",
    train=True,
    download=True,
    transform=train_transform,
)

test_ds = datasets.MNIST(
    root="data",
    train=False,
    download=True,
    transform=test_transform,
)

train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=128)</code></pre>
          <p class="small">Every time <code>train_loader</code> samples an image, a fresh random augmentation is applied.</p>
        </section>

        <section>
          <h3>Choosing Augmentations Carefully</h3>
          <ul>
            <li class="fragment">Valid if the transform does not change the label</li>
            <li class="fragment">Digits (MNIST): horizontal flips or 180° rotations can change a “6” into a “9” → avoid</li>
            <li class="fragment">Natural images: flips and small rotations are usually safe</li>
            <li class="fragment">Domain-specific tasks (medical, documents) require extra care and domain knowledge</li>
          </ul>
        </section>

        <section>
          <h3>When Does Augmentation Help Most?</h3>
          <ul>
            <li class="fragment">You have limited labeled data and a relatively large model</li>
            <li class="fragment">Test-time conditions are diverse: different devices, lighting, or viewpoints</li>
            <li class="fragment">You want robustness to small perturbations (cropping, noise, color shifts)</li>
            <li class="fragment">Often combined with Dropout/BatchNorm and early stopping</li>
          </ul>
        </section>

        <section>
          <h3>From LeNet to Modern CNNs</h3>
          <p class="small">Architecture design evolved, but core building blocks stayed similar.</p>
          <div class="timeline small">
            <ul>
              <li class="fragment"><strong>LeNet-5 (1998):</strong> small CNN for digits (MNIST-like); conv → pool → conv → pool → FC</li>
              <li class="fragment"><strong>AlexNet (2012):</strong> deeper CNN for ImageNet; ReLU, Dropout, trained on GPU</li>
              <li class="fragment"><strong>VGG (2014):</strong> very deep, simple stacks of \(3\times3\) convs and max pooling</li>
              <li class="fragment"><strong>Inception / GoogLeNet (2014):</strong> multi-branch “Inception” modules with different kernel sizes</li>
              <li class="fragment"><strong>ResNet (2015):</strong> residual connections (skip connections) enabled very deep networks</li>
            </ul>
          </div>
        </section>

        <section>
          <h3>Architecture Sketch: LeNet vs AlexNet</h3>
          <pre><code class="language-dot" data-graphviz>
digraph lenet_alexnet {
  rankdir=TB;
  node [fontsize=10, shape=box, style=rounded];

  subgraph cluster_lenet {
    label="LeNet-5 (digits)";
    style=dashed;
    l_in   [label="1×32×32 input"];
    l_c1   [label="Conv 6×5×5 + ReLU"];
    l_p1   [label="2×2 max pool"];
    l_c2   [label="Conv 16×5×5 + ReLU"];
    l_p2   [label="2×2 max pool"];
    l_fc1  [label="FC 120"];
    l_fc2  [label="FC 84"];
    l_out  [label="10-way output"];
    l_in -> l_c1 -> l_p1 -> l_c2 -> l_p2 -> l_fc1 -> l_fc2 -> l_out;
  }

  subgraph cluster_alex {
    label="AlexNet (ImageNet)";
    style=dashed;
    a_in   [label="3×224×224 input"];
    a_c1   [label="Conv + ReLU\n+ max pool"];
    a_c2   [label="Conv + ReLU\n+ max pool"];
    a_c3   [label="Conv"];
    a_c4   [label="Conv"];
    a_c5   [label="Conv + ReLU\n+ max pool"];
    a_fc1  [label="FC + ReLU\n+ Dropout"];
    a_fc2  [label="FC + ReLU\n+ Dropout"];
    a_out  [label="1000-way output"];
    a_in -> a_c1 -> a_c2 -> a_c3 -> a_c4 -> a_c5 -> a_fc1 -> a_fc2 -> a_out;
  }
}
          </code></pre>
          <p class="small">Both use conv + nonlinearity + pooling → fully connected layers; AlexNet is deeper and uses ReLU, Dropout, and GPUs.</p>
        </section>

        <section>
          <h3>VGG: Deep and Simple</h3>
          <ul>
            <li class="fragment">Key idea: stack many \(3\times3\) conv layers instead of a few large kernels</li>
            <li class="fragment">Pattern: \((\text{Conv} \rightarrow \text{ReLU})\) repeated 2–3 times → max pool → repeat</li>
            <li class="fragment">Very uniform design; easy to implement and reason about</li>
            <li class="fragment">Downside: many parameters and heavy computation</li>
          </ul>
          <pre><code class="language-python"># simplified VGG-style block
import torch.nn as nn

vgg_block = nn.Sequential(
    nn.Conv2d(64, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.Conv2d(64, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2),  # spatial size / 2
)</code></pre>
        </section>

        <section>
          <h3>Inception: Multi-Scale Features</h3>
          <p class="small">Inception modules apply multiple filter sizes in parallel and concatenate outputs.</p>
          <pre><code class="language-dot" data-graphviz>
digraph inception_block {
  rankdir=TB;
  node [fontsize=10, shape=box, style=rounded];

  input  [label="input feature map"];
  b1_1x1 [label="1×1 conv"];
  b2_1x1 [label="1×1 conv\n(reduce)"];
  b2_3x3 [label="3×3 conv"];
  b3_1x1 [label="1×1 conv\n(reduce)"];
  b3_5x5 [label="5×5 conv"];
  b4_pool [label="3×3 max pool"];
  b4_1x1 [label="1×1 conv"];
  concat [label="concat\nchannels"];

  input -> b1_1x1 -> concat;
  input -> b2_1x1 -> b2_3x3 -> concat;
  input -> b3_1x1 -> b3_5x5 -> concat;
  input -> b4_pool -> b4_1x1 -> concat;
}
          </code></pre>
          <p class="small">Network chooses useful features at multiple scales (1×1, 3×3, 5×5) in the same layer.</p>
        </section>

        <section>
          <h3>ResNet: Residual Connections</h3>
          <p class="small">Residual (skip) connections help train very deep networks by making layers learn a residual correction.</p>
          <pre><code class="language-dot" data-graphviz>
digraph res_block {
  rankdir=LR;
  node [fontsize=10, shape=box, style=rounded];

  x    [label="input x"];
  f    [label="Conv → BN → ReLU\nConv → BN"];
  add  [label="x + F(x)"];
  relu [label="ReLU"];

  x -> f -> add -> relu;
  x -> add [label="skip", style=dashed];
}
          </code></pre>
          <pre><code class="language-python">class BasicBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
        )
        self.relu = nn.ReLU()

    def forward(self, x):
        out = self.conv(x)
        out = out + x  # skip connection (same shape)
        return self.relu(out)</code></pre>
        </section>

        <section>
          <h3>Transfer Learning with CNNs</h3>
          <ul>
            <li class="fragment">Modern practice: start from a CNN pre-trained on a large dataset (e.g., ImageNet)</li>
            <li class="fragment">Freeze most convolutional layers; replace and train the final classifier head</li>
            <li class="fragment">Works well even with limited labeled data in your domain</li>
            <li class="fragment">We will revisit transfer learning in later modules</li>
          </ul>
        </section>

        <section>
          <h3>CNNs for Object Tracking</h3>
          <ul>
            <li class="fragment">Object tracking: follow a target across video frames (e.g., a car or person)</li>
            <li class="fragment">Typical pipeline: CNN features → predict new bounding box / similarity to target template</li>
            <li class="fragment">Augmentation is especially important (scale changes, motion blur, occlusions)</li>
            <li class="fragment">Example project: <a href="https://github.com/turhancan97/Convolutional-Neural-Network-for-Object-Tracking" target="_blank">CNN for Object Tracking (GitHub)</a></li>
          </ul>
        </section>

        <section>
          <h3>Switch to Notebook / Code</h3>
          <ul>
            <li class="fragment">Add data augmentation to your existing MNIST / Fashion-MNIST CNN training code</li>
            <li class="fragment">Visualize a few random augmented samples per batch to build intuition</li>
            <li class="fragment">Experiment with slightly deeper CNNs (VGG-style blocks, small residual blocks)</li>
          </ul>
        </section>

        <section>
          <h3>Homework</h3>
          <ul class="small">
            <li class="fragment">Implement and compare different augmentation strategies (none vs flips vs flips+rotations vs color jitter) on MNIST or Fashion-MNIST.</li>
            <li class="fragment">Train at least two CNN architectures (e.g., baseline from Lesson 3 vs a deeper VGG-style or residual model) and compare validation performance.</li>
            <li class="fragment">Plot training and validation curves for each setup; discuss how augmentation and architecture depth affected overfitting.</li>
            <li class="fragment">Optional: explore the object tracking repository linked in the README and identify where CNN features are used.</li>
          </ul>
        </section>

      </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/viz.js@2.1.2/viz.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/viz.js@2.1.2/full.render.js"></script>
    <script src="assets/slides-common.js"></script>
    <script>
      initializeSlides({
        width: 1536,
        height: 864,
      });
    </script>
  </body>
</html>
