<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lesson 4 — Data Augmentation & CNN Architectures</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/theme/white.css" id="theme">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/highlight/monokai.css">
    <link rel="stylesheet" href="assets/slides-common.css">
    <style>
      /* Lesson 4 specific overrides */
      .code-small pre code { font-size: 0.8em; }
      .two-column { display: flex; gap: 2rem; align-items: flex-start; }
      .two-column > div { flex: 1; }
      .timeline ul { list-style: none; padding-left: 0; }
      .timeline li { margin: 0.25rem 0; }
    </style>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h2>Lesson 4: Data Augmentation & CNN Architectures</h2>
          <h3>Learning Objectives</h3>
          <ul>
            <li class="fragment">Understand why and when we use data augmentation for vision models</li>
            <li class="fragment">Recognize common augmentation operations and how to implement them in PyTorch</li>
            <li class="fragment">Build intuition for the evolution of CNN architectures from LeNet to ResNet</li>
            <li class="fragment">Connect CNN features to downstream tasks like object tracking</li>
          </ul>
        </section>

        <section>
          <h3>Recap: CNNs and Overfitting</h3>
          <ul>
            <li class="fragment">CNNs can have millions of parameters and easily memorize the training set</li>
            <li class="fragment">Overfitting symptoms: training loss \(\downarrow\) while validation loss \(\uparrow\) or plateaus early</li>
            <li class="fragment">We already saw regularization like Dropout and stabilization techniques like BatchNorm</li>
            <li class="fragment">In this lesson we add <strong>weight decay</strong> and <strong>data augmentation</strong> to our toolkit</li>
          </ul>
        </section>

        <section>
          <h3>Regularization with Weight Decay</h3>
          <p class="small">Weight decay (L2 regularization) discourages very large weights by adding a penalty to the loss.</p>
          <p class="small">\[
            L_{\text{total}} = L_{\text{data}} + \lambda \lVert W \rVert_2^2
          \]</p>
          <ul class="small">
            <li class="fragment">\(\lambda\) controls the strength of the penalty (e.g., \(10^{-4}\) or \(10^{-3}\))</li>
            <li class="fragment">Encourages simpler models and can reduce overfitting, especially with many parameters</li>
            <li class="fragment">In PyTorch, set <code>weight_decay</code> in the optimizer instead of manually modifying the loss</li>
          </ul>
          <pre><code class="language-python">import torch.optim as optim

optimizer = optim.Adam(model.parameters(),
                       lr=1e-3,
                       weight_decay=1e-4)  # L2 penalty</code></pre>
          <p class="small">Combine weight decay with Dropout, BatchNorm, and data augmentation for more robust CNNs.</p>
        </section>

        <section>
          <h3>Idea: Data Augmentation</h3>
          <p class="small">Data augmentation creates new training examples by applying label-preserving transformations.</p>
          <ul>
            <li class="fragment">Example: flipping, rotating, cropping, or color-jittering images</li>
            <li class="fragment">We do <em>not</em> change the label — a rotated cat is still a cat</li>
            <li class="fragment">Model learns to be robust to common variations (pose, lighting, small translations)</li>
            <li class="fragment">Acts like training on a much larger dataset without collecting more images</li>
          </ul>
        </section>

        <section>
          <h3>Types of Augmentation</h3>
          <div class="two-column small">
            <div>
              <strong>Geometric</strong>
              <ul>
                <li class="fragment">Random crop / resize</li>
                <li class="fragment">Horizontal / vertical flip</li>
                <li class="fragment">Small rotations (e.g., \(\pm 10^\circ\))</li>
                <li class="fragment">Random affine transforms (scale, shear)</li>
              </ul>
            </div>
            <div>
              <strong>Photometric</strong>
              <ul>
                <li class="fragment">Brightness / contrast changes</li>
                <li class="fragment">Color jitter (hue, saturation)</li>
                <li class="fragment">Gaussian noise or blur</li>
                <li class="fragment">Random grayscale</li>
              </ul>
            </div>
          </div>
          <ul class="small">
            <li class="fragment">Choose transforms that make sense for your data and task (e.g., avoid vertical flips for digits)</li>
          </ul>
        </section>

        <section>
          <h3>Data Augmentation Pipeline</h3>
          <pre><code class="language-dot" data-graphviz>
digraph aug_pipeline {
  rankdir=LR;
  node [fontsize=11, shape=box, style=rounded];

  raw   [label="Original image\n(train sample)"];
  aug   [label="Random transforms\n(flip, rotate, color jitter)"];
  batch [label="Augmented batch\n(x_batch, y_batch)"];
  model [label="CNN model\n(f(x; θ))"];

  raw -> aug -> batch -> model;
}
          </code></pre>
          <p class="small">Each epoch, the same image can look different → the model sees a stream of varied views.</p>
        </section>

        <section>
          <h3>PyTorch: Augmentation with <code>torchvision.transforms</code></h3>
          <pre><code class="language-python">import torchvision.transforms as T

train_transform = T.Compose([
    T.RandomHorizontalFlip(p=0.5),
    T.RandomRotation(degrees=10),
    T.ColorJitter(brightness=0.2, contrast=0.2),
    T.ToTensor(),
    T.Normalize((0.1307,), (0.3081,)),  # MNIST stats
])

test_transform = T.Compose([
    T.ToTensor(),
    T.Normalize((0.1307,), (0.3081,)),
])</code></pre>
          <p class="small">Apply <code>train_transform</code> only to training data; keep validation/test transforms deterministic.</p>
        </section>

        <section>
          <h3>Using Transforms in a Dataset</h3>
          <pre><code class="language-python">from torchvision import datasets
from torch.utils.data import DataLoader

train_ds = datasets.MNIST(
    root="data",
    train=True,
    download=True,
    transform=train_transform,
)

test_ds = datasets.MNIST(
    root="data",
    train=False,
    download=True,
    transform=test_transform,
)

train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=128)</code></pre>
          <p class="small">Every time <code>train_loader</code> samples an image, a fresh random augmentation is applied.</p>
        </section>

        <section>
          <h3>Choosing Augmentations Carefully</h3>
          <ul>
            <li class="fragment">Valid if the transform does not change the label</li>
            <li class="fragment">Digits (MNIST): horizontal flips or 180° rotations can change a “6” into a “9” → avoid</li>
            <li class="fragment">Natural images: flips and small rotations are usually safe</li>
            <li class="fragment">Domain-specific tasks (medical, documents) require extra care and domain knowledge</li>
          </ul>
        </section>

        <section>
          <h3>When Does Augmentation Help Most?</h3>
          <ul>
            <li class="fragment">You have limited labeled data and a relatively large model</li>
            <li class="fragment">Test-time conditions are diverse: different devices, lighting, or viewpoints</li>
            <li class="fragment">You want robustness to small perturbations (cropping, noise, color shifts)</li>
            <li class="fragment">Often combined with Dropout/BatchNorm and early stopping</li>
          </ul>
        </section>

        <section>
          <h3>From LeNet to Modern CNNs</h3>
          <p class="small">Architecture design evolved, but core building blocks stayed similar.</p>
          <div class="timeline small">
            <ul>
              <li class="fragment"><strong>LeNet-5 (1998):</strong> small CNN for digits (MNIST-like); conv → pool → conv → pool → FC</li>
              <li class="fragment"><strong>AlexNet (2012):</strong> deeper CNN for ImageNet; ReLU, Dropout, trained on GPU</li>
              <li class="fragment"><strong>VGG (2014):</strong> very deep, simple stacks of \(3\times3\) convs and max pooling</li>
              <li class="fragment"><strong>Inception / GoogLeNet (2014):</strong> multi-branch “Inception” modules with different kernel sizes</li>
              <li class="fragment"><strong>ResNet (2015):</strong> residual connections (skip connections) enabled very deep networks</li>
            </ul>
          </div>
        </section>

        <section>
          <h3>AlexNet: Bigger CNN for ImageNet</h3>
          <ul>
            <li class="fragment"><strong>Problem:</strong> ImageNet has millions of RGB images and 1000 classes — LeNet is too small and shallow.</li>
            <li class="fragment"><strong>Key ideas:</strong> deeper conv stack (5 conv + 3 FC), ReLU everywhere, Dropout in fully connected layers, trained on GPUs.</li>
            <li class="fragment"><strong>What it solved:</strong> showed CNNs can scale to large, real-world datasets and dramatically beat hand-crafted features.</li>
            <li class="fragment"><strong>Trade-offs:</strong> large model (tens of millions of parameters), heavy compute and memory usage.</li>
          </ul>
        </section>

        <section>
          <h3>AlexNet Architecture (Diagram)</h3>
          <p class="small">High-level view of the original AlexNet layers (input, conv / pooling blocks, fully connected head).</p>
          <div class="img-row">
            <img src="assets/alexnet-architecture.jpg" alt="AlexNet architecture diagram" />
          </div>
        </section>

        <section>
          <h3>Architecture Sketch: LeNet vs AlexNet</h3>
          <div class="img-row">
            <img src="assets/lenet_AlexNet_block_diagram.svg" alt="LeNet vs AlexNet block diagram" />
          </div>
        </section>

        <section>
          <h3>LeNet vs AlexNet (Summary)</h3>
          <p class="small">Both use conv + nonlinearity + pooling → fully connected layers; AlexNet scales this pattern up for large-scale vision.</p>
          <ul class="small">
            <li class="fragment"><strong>Why AlexNet?</strong> ImageNet is much larger and more varied than MNIST, so we need a deeper, higher-capacity CNN.</li>
            <li class="fragment"><strong>How:</strong> more conv layers, many more channels, aggressive pooling, and heavy use of ReLU + Dropout.</li>
            <li class="fragment"><strong>Key changes vs LeNet:</strong> supports RGB images, trains on GPUs, and scales up width/depth to handle 1000-way classification.</li>
          </ul>
        </section>

        <section>
          <h3>VGG: Deep and Simple</h3>
          <ul>
            <li class="fragment">Key idea: stack many \(3\times3\) conv layers instead of a few large kernels</li>
            <li class="fragment">Pattern: \((\text{Conv} \rightarrow \text{ReLU})\) repeated 2–3 times → max pool → repeat</li>
            <li class="fragment"><strong>Why:</strong> deeper networks capture more complex patterns; keeping blocks identical simplifies design and tuning.</li>
            <li class="fragment"><strong>Changes vs AlexNet:</strong> replaces large, varied kernels with repeated \(3\times3\) convs, trading width for depth and regular structure.</li>
            <li class="fragment">Downside: many parameters and heavy computation (expensive without modern accelerators).</li>
          </ul>
          <pre><code class="language-python"># simplified VGG-style block
import torch.nn as nn

vgg_block = nn.Sequential(
    nn.Conv2d(64, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.Conv2d(64, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2),  # spatial size / 2
)</code></pre>
        </section>

        <section>
          <h3>Inception: Multi-Scale Features</h3>
          <p class="small">Inception modules apply multiple filter sizes in parallel and concatenate outputs.</p>
          <pre><code class="language-dot" data-graphviz>
digraph inception_block {
  rankdir=TB;
  node [fontsize=10, shape=box, style=rounded];

  input  [label="input feature map"];
  b1_1x1 [label="1×1 conv"];
  b2_1x1 [label="1×1 conv\n(reduce)"];
  b2_3x3 [label="3×3 conv"];
  b3_1x1 [label="1×1 conv\n(reduce)"];
  b3_5x5 [label="5×5 conv"];
  b4_pool [label="3×3 max pool"];
  b4_1x1 [label="1×1 conv"];
  concat [label="concat\nchannels"];

  input -> b1_1x1 -> concat;
  input -> b2_1x1 -> b2_3x3 -> concat;
  input -> b3_1x1 -> b3_5x5 -> concat;
  input -> b4_pool -> b4_1x1 -> concat;
}
          </code></pre>
          <p class="small">Network chooses useful features at multiple scales (1×1, 3×3, 5×5) in the same layer.</p>
          <ul class="small">
            <li class="fragment"><strong>Why:</strong> different objects and patterns appear at different scales; a single kernel size can miss useful structure.</li>
            <li class="fragment"><strong>How:</strong> parallel branches with 1×1, 3×3, 5×5 convs and pooling, plus 1×1 “bottlenecks” to keep compute affordable.</li>
            <li class="fragment"><strong>Changes vs VGG:</strong> moves from a single conv path per block to multi-branch modules that learn multi-scale features in one stage.</li>
          </ul>
        </section>

        <section>
          <h3>ResNet: Residual Connections</h3>
          <p class="small">Residual (skip) connections help train very deep networks by making layers learn a residual correction.</p>
          <div class="img-row">
            <img src="assets/residual_blocks.png" alt="Residual block diagram" />
          </div>
          <ul class="small">
            <li class="fragment"><strong>Why:</strong> very deep plain networks are hard to train (vanishing gradients, degradation — deeper nets perform worse).</li>
            <li class="fragment"><strong>How:</strong> each block learns a residual \(F(x)\); the skip connection lets gradients flow directly through \(x\).</li>
          </ul>
        </section>

        <section>
          <h3>ResNet: Going Deeper</h3>
          <p class="small">Stacking many residual blocks with skip connections makes 50–100+ layer CNNs trainable in practice.</p>
          <div class="img-row">
            <img height="350" src="assets/resnet.png" alt="ResNet architecture overview" />
          </div>
          <ul class="small">
            <li class="fragment"><strong>Changes vs Inception/VGG:</strong> focuses on depth with identity shortcuts instead of complex multi-branch modules.</li>
            <li class="fragment">Residual paths act like “highways” for gradients, reducing vanishing-gradient issues in very deep models.</li>
          </ul>
        </section>

        <section>
          <h3>Code: Simple Residual Block (PyTorch)</h3>
          <pre><code class="language-python">class BasicBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
        )
        self.relu = nn.ReLU()

    def forward(self, x):
        out = self.conv(x)
        out = out + x  # skip connection (same shape)
        return self.relu(out)</code></pre>
          <p class="small">Here the block learns a residual \(F(x)\) and adds it back to the original input \(x\) before the final ReLU.</p>
        </section>

        <section>
          <h3>Transfer Learning with CNNs</h3>
          <ul>
            <li class="fragment">Modern practice: start from a CNN pre-trained on a large dataset (e.g., ImageNet)</li>
            <li class="fragment">Freeze most convolutional layers; replace and train the final classifier head</li>
            <li class="fragment">Works well even with limited labeled data in your domain</li>
            <li class="fragment">In this lesson we implement transfer learning with data augmentation in the companion notebook</li>
          </ul>
        </section>

        <section>
          <h3>CNNs for Object Tracking</h3>
          <ul>
            <li class="fragment">Object tracking: follow a target across video frames (e.g., a car or person)</li>
            <li class="fragment">Typical pipeline: CNN features → predict new bounding box / similarity to target template</li>
            <li class="fragment">Augmentation is especially important (scale changes, motion blur, occlusions)</li>
            <li class="fragment">Example project: <a href="https://github.com/turhancan97/Convolutional-Neural-Network-for-Object-Tracking" target="_blank">CNN for Object Tracking (GitHub)</a></li>
          </ul>
        </section>

        <section>
          <h3>Switch to Notebook / Code</h3>
          <ul>
            <li class="fragment">Open <code>notebooks/lesson4_data_augmentation.ipynb</code></li>
            <li class="fragment">Add and visualize data augmentation for your training images</li>
            <li class="fragment">Apply transfer learning with a pre-trained CNN (e.g., ResNet) on a smaller dataset</li>
            <li class="fragment">Compare results with and without augmentation / fine-tuning</li>
          </ul>
        </section>

        <section>
          <h3>Homework</h3>
          <ul class="small">
            <li class="fragment">Implement and compare different augmentation strategies (none vs flips vs flips+rotations vs color jitter) on MNIST or Fashion-MNIST.</li>
            <li class="fragment">Train at least two CNN architectures (e.g., baseline from Lesson 3 vs a deeper VGG-style or residual model) and compare validation performance.</li>
            <li class="fragment">Plot training and validation curves for each setup; discuss how augmentation and architecture depth affected overfitting.</li>
            <li class="fragment">Optional: explore the object tracking repository linked in the README and identify where CNN features are used.</li>
          </ul>
        </section>

      </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/viz.js@2.1.2/viz.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/viz.js@2.1.2/full.render.js"></script>
    <script src="assets/slides-common.js"></script>
    <script>
      initializeSlides({
        width: 1536,
        height: 864,
      });
    </script>
  </body>
</html>
