<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lesson 5 — Variational Autoencoders & Latent PCA</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/theme/white.css" id="theme">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/highlight/monokai.css">
    <link rel="stylesheet" href="assets/slides-common.css">
    <style>
      .code-small pre code { font-size: 0.8em; }
      .two-column { display: flex; gap: 2rem; align-items: flex-start; }
      .two-column > div { flex: 1; }
      .small { font-size: 0.9em; }
    </style>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h2>Lesson 5: Variational Autoencoders & Latent PCA</h2>
          <h3>Learning Objectives</h3>
          <ul>
            <li class="fragment">Understand how the convolutional VAE in <code>projects/face_autoencoder/src/model.py</code> encodes and decodes face images.</li>
            <li class="fragment">Explain the reparameterization trick and the \(\beta\)-ELBO loss implemented in <code>VAELoss</code>.</li>
            <li class="fragment">Collect latent representations, run PCA, and use the Gradio sliders in <code>app.py</code> to traverse interpretable directions.</li>
          </ul>
        </section>

        <section>
          <h3>Where the VAE Fits</h3>
          <p class="small">We train a generative model that learns a smooth latent space of 64x64 RGB faces and supports interactive exploration.</p>
          <pre><code class="language-dot" data-graphviz>
digraph vae_flow {
  rankdir=LR;
  node [fontsize=11, shape=box, style=rounded];

  images   [label="Input faces\n(x) [64x64x3]"];
  encoder  [label="Encoder CNN\nstrided conv blocks"];
  heads    [label="Latent heads\nfc_mu & fc_logvar"];
  sample   [label="Reparameterize\nmu + sigma * eps"];
  decoder  [label="Decoder CNN\nConvTranspose + Tanh"];
  recon    [label="Reconstruction\n~x"];
  pca      [label="Latent PCA\nmetadata + sliders"];

  images -> encoder -> heads -> sample -> decoder -> recon;
  heads -> pca [style=dashed];
  sample -> pca [style=dashed];
}
          </code></pre>
          <ul class="small">
            <li class="fragment">The encoder compresses spatial structure; latent heads model a Gaussian posterior; the decoder mirrors the encoder.</li>
            <li class="fragment">Latent samples feed both reconstruction training and the PCA analysis used later in the app.</li>
          </ul>
        </section>

        <section>
          <h3>Encoder Anatomy</h3>
          <div class="two-column small">
            <div>
              <ul>
                <li class="fragment">Four `_conv_block` stages progressively downsample 64 -> 32 -> 16 -> 8 -> 4 while increasing channels (32 -> 256).</li>
                <li class="fragment">Each block: `Conv2d(stride=2)` + `BatchNorm2d` + `LeakyReLU(0.2)` for stable feature scaling and nonlinearity.</li>
                <li class="fragment">A dummy tensor at init time infers the flattened dimension so we can support arbitrary image sizes.</li>
              </ul>
            </div>
            <div class="code-small">
              <pre><code class="language-python">self.encoder_cnn = nn.Sequential(
    _conv_block(image_channels, base_channels),
    _conv_block(base_channels, base_channels * 2),
    _conv_block(base_channels * 2, base_channels * 4),
    _conv_block(base_channels * 4, base_channels * 8),
)

h = self.encoder_cnn(x)
h = h.view(x.size(0), -1)
              </code></pre>
            </div>
          </div>
        </section>

        <section>
          <h3>Latent Gaussian + Reparameterization</h3>
          <div class="two-column small">
            <div>
              <ul>
                <li class="fragment">Two linear heads (`fc_mu`, `fc_logvar`) map flattened features to the mean and log-variance of \(q_\phi(z \mid x) = \mathcal{N}(\mu, \mathrm{diag}(\sigma^2))\).</li>
                <li class="fragment">`reparameterize` keeps gradients flowing by sampling \(\epsilon \sim \mathcal{N}(0, I)\) and computing \(z = \mu + \sigma \odot \epsilon\).</li>
                <li class="fragment">`logvar` is stored instead of \(\sigma\) to ensure positivity via `torch.exp(0.5 * logvar)`.</li>
              </ul>
            </div>
            <div class="code-small">
              <pre><code class="language-python">def encode(self, x):
    h = self.encoder_cnn(x).view(x.size(0), -1)
    mu = self.fc_mu(h)
    logvar = self.fc_logvar(h)
    return mu, logvar

def reparameterize(self, mu, logvar):
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    return mu + eps * std
              </code></pre>
            </div>
          </div>
        </section>

        <section>
          <h3>Decoder & Reconstruction Quality</h3>
          <ul>
            <li class="fragment">`decoder_input` projects each latent vector back to the 4x4xC tensor inferred from the encoder.</li>
            <li class="fragment">Three `_deconv_block` stages mirror the encoder by doubling spatial size via `ConvTranspose2d` strides.</li>
            <li class="fragment">A final `ConvTranspose2d -> Tanh` predicts RGB pixels scaled to [-1, 1]; downstream utilities denormalize for visualization.</li>
          </ul>
          <pre class="code-small"><code class="language-python">def decode(self, z):
    h = self.decoder_input(z)
    h = h.view(z.size(0), self.enc_channels, self.enc_spatial, self.enc_spatial)
    return self.decoder_cnn(h)
          </code></pre>
        </section>

        <section>
          <h3>\(\beta\)-ELBO Loss</h3>
          <ul class="small">
            <li class="fragment">Objective: maximize the Evidence Lower Bound \(\mathcal{L} = \mathbb{E}[\log p_\theta(x \mid z)] - \beta \; \mathrm{KL}(q_\phi(z \mid x) \Vert p(z))\).</li>
            <li class="fragment">`VAELoss` uses `F.mse_loss` for the reconstruction term (faces are continuous) and a closed-form KL between diagonal Gaussians.</li>
            <li class="fragment">Tuning \(\beta\) trades sharp reconstructions (low \(\beta\)) for more disentangled latents (high \(\beta\)).</li>
          </ul>
          <pre class="code-small"><code class="language-python">recon_loss = F.mse_loss(output.reconstruction, target, reduction="mean")
kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
loss = recon_loss + beta * kl_div
          </code></pre>
        </section>

        <section>
          <h3>Training Loop Helpers (<code>training_utils.py</code>)</h3>
          <div class="two-column small">
            <div>
              <ul>
                <li class="fragment">`seed_everything` and callbacks are small utilities that make VAE experiments reproducible and easy to monitor.</li>
                <li class="fragment">Latent vectors \(\mu(x)\) collected from the encoder can be analyzed with PCA to find major directions of variation.</li>
                <li class="fragment">A simple Gradio app can then expose a few PCA directions as sliders to explore the learned latent space interactively.</li>
              </ul>
            </div>
            <div class="code-small">
              <p class="small fragment"><strong>Case study (optional):</strong> inspect <code>training_utils.py</code> and <code>app.py</code> in the project for a concrete implementation of these ideas.</p>
            </div>
          </div>
        </section>

        <section>
          <h3>Plain Autoencoder (Deterministic)</h3>
          <p class="small">A basic autoencoder compresses and reconstructs data without any probabilistic latent space.</p>
          <pre><code class="language-dot" data-graphviz>
digraph ae_flow {
  rankdir=LR;
  node [fontsize=11, shape=box, style=rounded];

  x      [label="Input image\nx"];
  enc    [label="Encoder\nConv / Linear layers"];
  z      [label="Latent code\nz"];
  dec    [label="Decoder\nConvTranspose / Linear"];
  x_hat  [label="Reconstruction\nx̂"];

  x -> enc -> z -> dec -> x_hat;
}
          </code></pre>
          <ul class="small">
            <li class="fragment">Train by minimizing reconstruction loss (e.g., MSE) between \(x\) and \(\hat{x}\).</li>
            <li class="fragment">Latent code \(z\) is a deterministic function of \(x\) with no explicit prior.</li>
            <li class="fragment">Works well for compression and denoising, but latent space may have gaps and is not guaranteed to be smooth.</li>
          </ul>
        </section>

        <section>
          <h3>Autoencoders on MNIST / Fashion-MNIST</h3>
          <p class="small">Before VAEs, it is useful to build intuition with a small, deterministic autoencoder on simple 28x28 images.</p>
          <ul class="small">
            <li class="fragment">Datasets: <code>MNIST</code> (digits) and <code>Fashion-MNIST</code> (clothing items).</li>
            <li class="fragment">Goal: learn a compact latent code that can reconstruct inputs and optionally denoise simple corruptions.</li>
            <li class="fragment">Architecture: 2–3 fully connected or convolutional layers in the encoder and a mirrored decoder.</li>
          </ul>
          <pre class="code-small"><code class="language-python">class MnistAutoencoder(nn.Module):
    def __init__(self, latent_dim=32):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28 * 28, 256), nn.ReLU(),
            nn.Linear(256, latent_dim),
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256), nn.ReLU(),
            nn.Linear(256, 28 * 28), nn.Sigmoid(),
        )

    def forward(self, x):
        z = self.encoder(x)
        x_hat = self.decoder(z).view(-1, 1, 28, 28)
        return x_hat</code></pre>
          <p class="small fragment">We will use this style of model for the homework on MNIST and Fashion-MNIST.</p>
        </section>

        <section>
          <h3>VAE vs Regular Autoencoder</h3>
          <div class="two-column small">
            <div>
              <h4>Regular Autoencoder</h4>
              <ul>
                <li class="fragment">Encoder: \(x \to z\) (deterministic)</li>
                <li class="fragment">Decoder: \(z \to \hat{x}\)</li>
                <li class="fragment">Latent space: unconstrained; may be irregular with “holes”.</li>
                <li class="fragment">Problem: sampling random \(z\) often lands off the data manifold → poor or meaningless generations.</li>
              </ul>
            </div>
            <div>
              <h4>Variational Autoencoder</h4>
              <ul>
                <li class="fragment">Encoder: \(x \to (\mu, \sigma)\) (probabilistic)</li>
                <li class="fragment">Sample: \(z \sim \mathcal{N}(\mu, \sigma^2)\)</li>
                <li class="fragment">Decoder: \(z \to \hat{x}\)</li>
                <li class="fragment">Latent space: continuous, smooth, generative</li>
              </ul>
            </div>
          </div>
          <p class="small fragment">VAEs learn a <em>distribution</em> over latents, enabling smooth interpolation and generation of new faces.</p>
        </section>

        <section>
          <h3>From Autoencoder Loss to VAE Loss</h3>
          <ul class="small">
            <li class="fragment"><strong>Autoencoder objective:</strong> minimize reconstruction error only, e.g. \(L_{\text{AE}} = \mathbb{E}_{x}[\|x - \hat{x}\|^2]\).</li>
            <li class="fragment">No explicit constraint on where latent codes \(z\) live → irregular latent space and poor samples from random \(z\).</li>
            <li class="fragment"><strong>VAE objective:</strong> keep reconstruction term, but add a KL regularizer that pulls \(q_\phi(z \mid x)\) toward a simple prior \(p(z)\).</li>
            <li class="fragment">This gives the Evidence Lower Bound (ELBO): \(\mathcal{L} = \mathbb{E}[\log p_\theta(x \mid z)] - \beta \, \mathrm{KL}(q_\phi(z \mid x) \Vert p(z))\).</li>
          </ul>
        </section>

        <section>
          <h3>VAE Generative Story</h3>
          <ul class="small">
            <li class="fragment"><strong>Prior:</strong> choose a simple latent distribution \(p(z) = \mathcal{N}(0, I)\).</li>
            <li class="fragment"><strong>Decoder:</strong> learn \(p_\theta(x \mid z)\) (e.g., Gaussian with mean given by a neural network) to reconstruct data from latent codes.</li>
            <li class="fragment"><strong>Encoder:</strong> learn \(q_\phi(z \mid x)\) that maps each example to a latent Gaussian \(\mathcal{N}(\mu(x), \sigma^2(x))\).</li>
            <li class="fragment">At test time we can either <em>encode–decode</em> (like an autoencoder) or <em>sample</em> \(z \sim p(z)\) and decode to generate new examples.</li>
          </ul>
        </section>

        <section>
          <h3>Step-by-Step: AE → VAE</h3>
          <div class="two-column small">
            <div>
              <h4>Plain Autoencoder</h4>
              <ul>
                <li class="fragment">Input \(x\)</li>
                <li class="fragment">Encoder outputs latent code \(z\)</li>
                <li class="fragment">Decoder outputs \(\hat{x}\)</li>
                <li class="fragment">Loss: reconstruction only (e.g., MSE)</li>
                <li class="fragment">Sampling: ad hoc; random \(z\) may not decode to realistic data</li>
              </ul>
            </div>
            <div>
              <h4>Variational Autoencoder</h4>
              <ul>
                <li class="fragment">Input \(x\)</li>
                <li class="fragment">Encoder outputs \((\mu(x), \log \sigma^2(x))\)</li>
                <li class="fragment">Sample \(z = \mu + \sigma \odot \epsilon\), \(\epsilon \sim \mathcal{N}(0, I)\)</li>
                <li class="fragment">Decoder outputs \(\hat{x}\)</li>
                <li class="fragment">Loss: reconstruction + KL to prior → well-structured latent space</li>
              </ul>
            </div>
          </div>
        </section>

        <section>
          <h3>Homework: Train an Autoencoder on MNIST & Fashion-MNIST</h3>
          <ul class="small">
            <li class="fragment">Implement a small PyTorch autoencoder (MLP or shallow CNN) for 28x28 grayscale images, following the <code>MnistAutoencoder</code> pattern.</li>
            <li class="fragment">Train on <code>MNIST</code> until reconstruction loss plateaus; visualize a grid of original vs reconstructed digits.</li>
            <li class="fragment">Repeat training on <code>Fashion-MNIST</code>; compare what kinds of structure the model captures (edges, shapes, textures).</li>
            <li class="fragment">Optionally add Gaussian noise to inputs and train a denoising autoencoder; report how this changes visual quality.</li>
            <li class="fragment">Reflect: how does a deterministic autoencoder's latent space differ from the VAE latent space used in the face project?</li>
          </ul>
        </section>

        <section>
          <h3>Why Reparameterization Trick?</h3>
          <ul class="small">
            <li class="fragment">Problem: sampling \(z \sim \mathcal{N}(\mu, \sigma^2)\) is non-differentiable</li>
            <li class="fragment">Direct sampling breaks backpropagation: gradients can't flow through random operations</li>
            <li class="fragment">Solution: separate randomness from learnable parameters</li>
          </ul>
          <div class="two-column small">
            <div>
              <p class="fragment"><strong>Without reparameterization:</strong></p>
              <pre class="code-small"><code class="language-python"># ❌ Can't backprop through this
z = torch.normal(mu, std)
# mu and std have no gradients!</code></pre>
            </div>
            <div>
              <p class="fragment"><strong>With reparameterization:</strong></p>
              <pre class="code-small"><code class="language-python"># ✅ Gradients flow through mu, std
eps = torch.randn_like(std)  # fixed noise
z = mu + eps * std
# Both mu and std are differentiable!</code></pre>
            </div>
          </div>
          <p class="small fragment">Key insight: \(\epsilon\) is sampled once and fixed; gradients flow through \(\mu\) and \(\sigma\).</p>
        </section>

        <section>
          <h3>Understanding KL Divergence</h3>
          <p class="small">The KL term regularizes the learned posterior \(q_\phi(z \mid x)\) to match the prior \(p(z) = \mathcal{N}(0, I)\).</p>
          <p class="small">\[
            \mathrm{KL}(q_\phi(z \mid x) \Vert p(z)) = \frac{1}{2} \sum_{i=1}^d \left( \sigma_i^2 + \mu_i^2 - 1 - \log \sigma_i^2 \right)
          \]</p>
          <ul class="small">
            <li class="fragment">\(\sigma_i^2 + \mu_i^2 - 1\): penalizes large means and variances (pushes toward standard normal)</li>
            <li class="fragment">\(-\log \sigma_i^2\): prevents collapse (keeps variance from going to zero)</li>
            <li class="fragment">Without KL: encoder could map each \(x\) to a different region → no smooth latent space</li>
            <li class="fragment">With KL: encoder learns to use a shared, structured latent space</li>
          </ul>
        </section>

        <section>
          <h3>\(\beta\)-VAE: Controlling Disentanglement</h3>
          <p class="small">The \(\beta\) parameter in \(\mathcal{L} = \mathbb{E}[\log p_\theta(x \mid z)] - \beta \; \mathrm{KL}(q_\phi(z \mid x) \Vert p(z))\) controls the trade-off:</p>
          <div class="two-column small">
            <div>
              <h4>Low \(\beta\) (e.g., 0.1-0.5)</h4>
              <ul>
                <li class="fragment">Emphasizes reconstruction quality</li>
                <li class="fragment">Sharper, more detailed outputs</li>
                <li class="fragment">Less structured latent space</li>
                <li class="fragment">Latent dimensions may be correlated</li>
              </ul>
            </div>
            <div>
              <h4>High \(\beta\) (e.g., 2-10)</h4>
              <ul>
                <li class="fragment">Emphasizes latent regularization</li>
                <li class="fragment">More disentangled features</li>
                <li class="fragment">Smoother, more interpretable latents</li>
                <li class="fragment">May sacrifice some reconstruction quality</li>
              </ul>
            </div>
          </div>
          <p class="small fragment"><strong>Standard VAE:</strong> \(\beta = 1.0\) (balanced). <strong>\(\beta\)-VAE:</strong> \(\beta > 1\) for better disentanglement.</p>
        </section>

        <section>
          <h3>Reconstruction Loss: MSE vs Log(MSE) vs Perceptual</h3>
          <div class="two-column small">
            <div>
              <h4>MSE Loss (Standard)</h4>
              <ul>
                <li class="fragment">Pixel-wise squared error: \(\|x - \hat{x}\|^2\)</li>
                <li class="fragment">Simple, fast, differentiable</li>
                <li class="fragment">Problem: averages out details → blurry reconstructions</li>
                <li class="fragment">Large errors dominate gradients</li>
              </ul>
              <div class="fragment">
                <img src="assets/mse_face.png" alt="Blurry face reconstruction from MSE loss" style="max-width: 100%; margin-top: 0.5em; border: 1px solid #ddd;" />
                <p class="small" style="margin-top: 0.3em; font-style: italic;">Example: MSE loss produces blurry, averaged reconstructions</p>
              </div>
              <pre class="code-small fragment"><code class="language-python">recon_loss = F.mse_loss(
    output.reconstruction, 
    target
)</code></pre>
            </div>
            <div>
              <h4>Log(MSE) Loss (Alternative)</h4>
              <ul>
                <li class="fragment">\(\log(\|x - \hat{x}\|^2 + \epsilon)\)</li>
                <li class="fragment">Reweights gradients: \(\frac{1}{\text{MSE}} \cdot \frac{\partial \text{MSE}}{\partial \theta}\)</li>
                <li class="fragment">Large errors contribute less; small errors contribute more</li>
                <li class="fragment">Often produces sharper results than standard MSE</li>
                <li class="fragment"><strong>Valid alternative</strong> that sometimes outperforms MSE</li>
              </ul>
              <pre class="code-small fragment"><code class="language-python">mse = F.mse_loss(output.reconstruction, target)
recon_loss = torch.log(mse + 1e-8)
# Or: loss_func = VAELoss(beta=1.0, use_log_mse=True)
</code></pre>
            </div>
          </div>
        </section>

        <section>
          <h3>Reconstruction Loss: Perceptual Loss</h3>
          <div class="two-column small">
            <div>
              <h4>Perceptual Loss (Enhanced)</h4>
              <ul>
                <li class="fragment">Feature-space error: \(\|f(x) - f(\hat{x})\|^2\)</li>
                <li class="fragment">Uses pre-trained VGG features</li>
                <li class="fragment">Better preserves facial structure and details</li>
                <li class="fragment">Sharper, more realistic reconstructions</li>
                <li class="fragment">Most computationally expensive option</li>
              </ul>
            </div>
            <div>
              <pre class="code-small fragment"><code class="language-python"># Compare VGG features instead
recon_features = vgg(x)
target_features = vgg(target)
perceptual_loss = F.mse_loss(
    recon_features, 
    target_features
)

# Or use PerceptualVAELoss:
loss_func = PerceptualVAELoss(
    beta=1.0,
    feature_layer='relu3_3'
)
</code></pre>
              <p class="small fragment"><strong>Summary:</strong> MSE (standard) → Log(MSE) (often better) → Perceptual (best quality, slower)</p>
            </div>
          </div>
        </section>

        <section>
          <h3>Perceptual Loss: VGG Feature Layers</h3>
          <p class="small">Different VGG layers capture different levels of abstraction:</p>
          <ul class="small">
            <li class="fragment"><strong>relu1_2:</strong> Low-level (edges, colors) — too shallow for faces</li>
            <li class="fragment"><strong>relu2_2:</strong> Mid-early (textures, patterns) — good detail preservation</li>
            <li class="fragment"><strong>relu3_3:</strong> Mid-level (object parts, facial structures) — <em>recommended for 64-96px faces</em></li>
            <li class="fragment"><strong>relu4_3:</strong> High-level (semantic features) — may lose spatial detail for small images</li>
          </ul>
          <pre class="code-small fragment"><code class="language-python"># In PerceptualVAELoss
loss_func = PerceptualVAELoss(
    beta=1.0,
    perceptual_weight=1.0,
    mse_weight=0.1,  # Optional: hybrid approach
    feature_layer='relu3_3'  # Best for faces
)</code></pre>
          <p class="small fragment">For face images, <code>relu3_3</code> balances structure preservation with detail.</p>
        </section>

        <section>
          <h3>Architecture Design Choices</h3>
          <div class="two-column small">
            <div>
              <h4>Encoder</h4>
              <ul>
                <li class="fragment"><strong>Strided convolutions:</strong> Efficient downsampling (no pooling needed)</li>
                <li class="fragment"><strong>BatchNorm:</strong> Stabilizes training, allows higher learning rates</li>
                <li class="fragment"><strong>LeakyReLU(0.2):</strong> Prevents dead neurons, common in GANs/VAEs</li>
                <li class="fragment"><strong>Progressive channels:</strong> 32→64→128→256 captures hierarchical features</li>
              </ul>
            </div>
            <div>
              <h4>Decoder</h4>
              <ul>
                <li class="fragment"><strong>ConvTranspose2d:</strong> Learns upsampling (better than interpolation)</li>
                <li class="fragment"><strong>Mirror structure:</strong> Symmetric to encoder for balanced capacity</li>
                <li class="fragment"><strong>Tanh output:</strong> Maps to [-1, 1] matching normalized inputs</li>
                <li class="fragment"><strong>No final activation:</strong> Could use sigmoid for [0,1], but Tanh works well</li>
              </ul>
            </div>
          </div>
        </section>

        <section>
          <h3>Training Considerations</h3>
          <ul class="small">
            <li class="fragment"><strong>Learning rate:</strong> Start with 1e-3 to 2e-3; VAEs can be sensitive to LR</li>
            <li class="fragment"><strong>Batch size:</strong> Larger batches (256+) help stabilize BatchNorm and gradients</li>
            <li class="fragment"><strong>KL annealing:</strong> Start with \(\beta=0\), gradually increase to final value (helps early training)</li>
            <li class="fragment"><strong>Latent dimension:</strong> Too small → information bottleneck; too large → underutilized capacity</li>
            <li class="fragment"><strong>Monitoring:</strong> Watch both reconstruction loss (quality) and KL (regularization) separately</li>
            <li class="fragment"><strong>Early stopping:</strong> Stop when validation loss plateaus; VAEs can overfit to training faces</li>
          </ul>
          <pre class="code-small fragment"><code class="language-python"># Example: KL annealing
for epoch in range(epochs):
    beta = min(1.0, epoch / 50)  # Ramp up over 50 epochs
    loss_func.beta = beta
    learn.fit_one_cycle(1, lr_max=lr)</code></pre>
        </section>

        <section>
          <h3>Why PCA on Latent Space?</h3>
          <ul class="small">
            <li class="fragment"><strong>Interpretability:</strong> PCA finds orthogonal directions of maximum variance</li>
            <li class="fragment"><strong>Dimensionality reduction:</strong> First few components capture most variation</li>
            <li class="fragment"><strong>Controllable generation:</strong> Each component often corresponds to a semantic attribute (smile, pose, lighting)</li>
            <li class="fragment"><strong>Linear interpolation:</strong> Moving along a component is smooth and predictable</li>
          </ul>
          <div class="two-column small fragment">
            <div>
              <p><strong>PCA Components:</strong></p>
              <ul>
                <li>PC1: Often captures pose/angle</li>
                <li>PC2: Often captures expression</li>
                <li>PC3: Often captures lighting</li>
                <li>PC4+: More subtle variations</li>
              </ul>
            </div>
            <div>
              <p><strong>Explained Variance:</strong></p>
              <ul>
                <li>First 8 components: ~60-80% variance</li>
                <li>First 16 components: ~85-95% variance</li>
                <li>Remaining: fine details</li>
              </ul>
            </div>
          </div>
        </section>

        <section>
          <h3>Latent Space Properties</h3>
          <ul class="small">
            <li class="fragment"><strong>Smoothness:</strong> Nearby points in latent space → similar faces (enabled by KL regularization)</li>
            <li class="fragment"><strong>Completeness:</strong> Most of latent space decodes to valid faces (not just training examples)</li>
            <li class="fragment"><strong>Interpolation:</strong> Linear paths between latents produce smooth face morphing</li>
            <li class="fragment"><strong>Arithmetic:</strong> Can do "smiling face - neutral + angry" = new expression (if disentangled)</li>
          </ul>
          <pre class="code-small fragment"><code class="language-python"># Example: Interpolation
z1 = model.encode(face1)[0]  # mu for face1
z2 = model.encode(face2)[0]  # mu for face2

for alpha in [0.0, 0.25, 0.5, 0.75, 1.0]:
    z_interp = (1 - alpha) * z1 + alpha * z2
    face_interp = model.decode(z_interp)
    # Smooth morphing between faces!</code></pre>
        </section>

        <section>
          <h3>Common Issues & Solutions</h3>
          <div class="two-column small">
            <div>
              <h4>Problem: Blurry Reconstructions</h4>
              <ul>
                <li class="fragment">Try perceptual loss instead of MSE</li>
                <li class="fragment">Reduce \(\beta\) (less KL pressure)</li>
                <li class="fragment">Increase model capacity (more channels)</li>
                <li class="fragment">Check if latent dim is too small</li>
              </ul>
            </div>
            <div>
              <h4>Problem: Posterior Collapse</h4>
              <ul>
                <li class="fragment">KL → 0, encoder ignores input</li>
                <li class="fragment">Increase \(\beta\) gradually</li>
                <li class="fragment">Use KL annealing</li>
                <li class="fragment">Check decoder isn't too powerful</li>
              </ul>
            </div>
          </div>
          <div class="two-column small fragment">
            <div>
              <h4>Problem: Training Instability</h4>
              <ul>
                <li>Lower learning rate</li>
                <li>Gradient clipping</li>
                <li>Warm-up period</li>
                <li>Check data normalization</li>
              </ul>
            </div>
            <div>
              <h4>Problem: Poor Latent Structure</h4>
              <ul>
                <li>Increase \(\beta\) for disentanglement</li>
                <li>Use \(\beta\)-VAE (\(\beta > 1\))</li>
                <li>Train longer</li>
                <li>Check KL term is active</li>
              </ul>
            </div>
          </div>
        </section>

        <section>
          <h3>Next Steps</h3>
          <ul>
            <li class="fragment">Reproduce the training pipeline in <code>projects/face_autoencoder/face_autoencoder_training.ipynb</code> and compare \(\beta\) values.</li>
            <li class="fragment">Experiment with different latent dimensions or `base_channels`, and observe how PCA variance ratios change.</li>
            <li class="fragment">Try <code>PerceptualVAELoss</code> instead of <code>VAELoss</code> and compare reconstruction quality.</li>
            <li class="fragment">Extend the Gradio app with preset buttons (e.g., "add smile", "turn head") by saving curated coefficient vectors.</li>
            <li class="fragment">Implement latent interpolation: encode two faces, interpolate in latent space, decode to see morphing.</li>
            <li class="fragment">Optional: try ICA or t-SNE on the collected latents to contrast with PCA's linear assumptions.</li>
          </ul>
        </section>

      </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/viz.js@2.1.2/viz.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/viz.js@2.1.2/full.render.js"></script>
    <script src="assets/slides-common.js"></script>
    <script>
      initializeSlides({
        width: 1536,
        height: 864,
      });
    </script>
  </body>
</html>
