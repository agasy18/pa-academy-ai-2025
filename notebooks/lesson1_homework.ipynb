{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 Homework — Exploring Layers and Activations\n",
    "\n",
    "Goals:\n",
    "- Recreate the TensorFlow Playground experiments locally\n",
    "- Observe how depth, width, activation, and learning rate affect results\n",
    "- Summarize your findings concisely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "If packages are missing, uncomment the pip installs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def make_circles(n_samples=1500, factor=0.5, noise=0.15, random_state=1):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n_outer = n_samples // 2\n",
    "    n_inner = n_samples - n_outer\n",
    "    theta_outer = rng.uniform(0, 2*np.pi, n_outer)\n",
    "    theta_inner = rng.uniform(0, 2*np.pi, n_inner)\n",
    "    X_outer = np.c_[np.cos(theta_outer), np.sin(theta_outer)]\n",
    "    X_inner = factor * np.c_[np.cos(theta_inner), np.sin(theta_inner)]\n",
    "    X_outer += noise * rng.standard_normal(X_outer.shape)\n",
    "    X_inner += noise * rng.standard_normal(X_inner.shape)\n",
    "    X = np.vstack([X_outer, X_inner]).astype('float32')\n",
    "    y = np.r_[np.zeros(n_outer), np.ones(n_inner)].astype('float32')\n",
    "    return X, y\n",
    "\n",
    "X, y = make_circles(n_samples=1500, factor=0.5, noise=0.15, random_state=1)\n",
    "plt.figure(figsize=(4.5,4))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='RdBu', s=10, edgecolor='k')\n",
    "plt.title('Dataset (make_circles)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071746e",
   "metadata": {},
   "source": [
    "## Helper: build and train model\n",
    "Adjust layers, units, activation, optimizer, learning rate, and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(units=(8,8), activation='relu', lr=1e-3):\n",
    "    layers = []\n",
    "    in_features = 2\n",
    "    for u in units:\n",
    "        layers.append(nn.Linear(in_features, u))\n",
    "        if activation == 'relu':\n",
    "            layers.append(nn.ReLU())\n",
    "        elif activation == 'tanh':\n",
    "            layers.append(nn.Tanh())\n",
    "        elif activation == 'sigmoid':\n",
    "            layers.append(nn.Sigmoid())\n",
    "        else:\n",
    "            layers.append(nn.ReLU())\n",
    "        in_features = u\n",
    "    layers.append(nn.Linear(in_features, 1))\n",
    "    layers.append(nn.Sigmoid())\n",
    "    model = nn.Sequential(*layers)\n",
    "    model.lr = lr\n",
    "    return model\n",
    "\n",
    "def train_and_plot(model, X, y, epochs=20, title=''):\n",
    "    # simple train/validation split (80/20)\n",
    "    idx = np.arange(len(X))\n",
    "    np.random.shuffle(idx)\n",
    "    split = int(0.8 * len(X))\n",
    "    train_idx, val_idx = idx[:split], idx[split:]\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "    X_train_t = torch.from_numpy(X_train)\n",
    "    y_train_t = torch.from_numpy(y_train).unsqueeze(1)\n",
    "    X_val_t = torch.from_numpy(X_val)\n",
    "    y_val_t = torch.from_numpy(y_val).unsqueeze(1)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=getattr(model, 'lr', 1e-3))\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    train_loss, val_loss = [], []\n",
    "    train_acc, val_acc = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_train_t)\n",
    "        loss = loss_fn(preds, y_train_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            train_pred = (preds > 0.5).float()\n",
    "            train_acc_epoch = (train_pred.eq(y_train_t)).float().mean().item()\n",
    "\n",
    "            val_probs = model(X_val_t)\n",
    "            val_pred = (val_probs > 0.5).float()\n",
    "            val_loss_epoch = loss_fn(val_probs, y_val_t).item()\n",
    "            val_acc_epoch = (val_pred.eq(y_val_t)).float().mean().item()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        val_loss.append(val_loss_epoch)\n",
    "        train_acc.append(train_acc_epoch)\n",
    "        val_acc.append(val_acc_epoch)\n",
    "\n",
    "    # loss curves\n",
    "    fig, ax = plt.subplots(1,2, figsize=(9,3.8))\n",
    "    ax[0].plot(train_loss, label='train')\n",
    "    ax[0].plot(val_loss, label='val')\n",
    "    ax[0].set_title('Loss'); ax[0].legend()\n",
    "    ax[0].grid(True, alpha=0.3)\n",
    "    ax[1].plot(train_acc, label='train')\n",
    "    ax[1].plot(val_acc, label='val')\n",
    "    ax[1].set_title('Accuracy'); ax[1].legend()\n",
    "    ax[1].grid(True, alpha=0.3)\n",
    "    fig.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'accuracy': train_acc,\n",
    "        'val_accuracy': val_acc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c127a1fd",
   "metadata": {},
   "source": [
    "## Task 1 — Layers and Units\n",
    "Try 1 hidden layer vs 2 hidden layers; vary hidden units (e.g., 4, 8, 16).\n",
    "- Record your validation accuracy and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = build_model(units=(8,), activation='relu', lr=1e-3)\n",
    "h1 = train_and_plot(m1, X, y, epochs=20, title='1 layer, 8 units, ReLU')\n",
    "\n",
    "m2 = build_model(units=(8,8), activation='relu', lr=1e-3)\n",
    "h2 = train_and_plot(m2, X, y, epochs=20, title='2 layers, 8-8 units, ReLU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41dc354",
   "metadata": {},
   "source": [
    "## Task 2 — Activation Choices\n",
    "Compare ReLU vs Tanh vs Sigmoid for hidden layers.\n",
    "- Which converges faster? Any saturation issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for act in ['relu','tanh','sigmoid']:\n",
    "    m = build_model(units=(16,16), activation=act, lr=1e-3)\n",
    "    _ = train_and_plot(m, X, y, epochs=25, title=f'2 layers, 16-16, activation={act}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497d50b",
   "metadata": {},
   "source": [
    "## Task 3 — Learning Rate Sensitivity\n",
    "Try a range of learning rates (e.g., 1e-4, 1e-3, 1e-2, 1e-1).\n",
    "- What happens for very large values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b503497",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "    m = build_model(units=(8,8), activation='relu', lr=lr)\n",
    "    _ = train_and_plot(m, X, y, epochs=20, title=f'LR={lr}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b835d25",
   "metadata": {},
   "source": [
    "## Optional — Decision Regions\n",
    "Visualize learned decision boundaries for your favorite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2358944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regions(model, X, y):\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 220),\n",
    "        np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 220)\n",
    "    )\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()].astype('float32')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        grid_t = torch.from_numpy(grid)\n",
    "        probs = model(grid_t).detach().numpy().reshape(xx.shape)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.contourf(xx, yy, probs, levels=20, cmap='RdBu', alpha=0.6)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, cmap='RdBu', edgecolor='k', s=10)\n",
    "    plt.title('Decision regions')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5336b315",
   "metadata": {},
   "source": [
    "## Short Reflection\n",
    "Answer in your own words (2–4 sentences each):\n",
    "1. How does adding a second hidden layer change what the model can represent?\n",
    "2. When did you observe over- or under-fitting? What signaled it?\n",
    "3. Which activation worked best here, and why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d02fd0",
   "metadata": {},
   "source": [
    "## To-Do (stubs only — do not implement)\n",
    "These are optional extensions meant for practice. Leave them as TODOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a manual NumPy forward pass for a 2-layer network\n",
    "# (ReLU hidden, sigmoid output) on the same dataset.\n",
    "# Compare predictions qualitatively to the PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe3c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a function plot_regions_threshold(model, X, y, threshold=0.5)\n",
    "# that visualizes decision regions for different probability thresholds.\n",
    "# Try thresholds: 0.3, 0.5, 0.7 and note changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cbc764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Swap loss to mean-squared error (MSE) while keeping sigmoid output.\n",
    "# Train briefly and record any differences in convergence/accuracy vs BCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add L2 weight decay (kernel_regularizer) or a Dropout layer\n",
    "# and observe effects on training/validation curves.\n",
    "# Keep other hyperparameters the same for a fair comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
