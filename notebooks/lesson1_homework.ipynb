{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 Homework — Exploring Layers and Activations\n",
    "\n",
    "Goals:\n",
    "- Recreate the TensorFlow Playground experiments locally\n",
    "- Observe how depth, width, activation, and learning rate affect results\n",
    "- Summarize your findings concisely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "If packages are missing, uncomment the pip installs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow scikit-learn matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=1500, factor=0.5, noise=0.15, random_state=1)\n",
    "X = X.astype('float32'); y = y.astype('float32')\n",
    "plt.figure(figsize=(4.5,4))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='RdBu', s=10, edgecolor='k')\n",
    "plt.title('Dataset (make_circles)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: build and train model\n",
    "Adjust layers, units, activation, optimizer, learning rate, and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(units=(8,8), activation='relu', lr=1e-3):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Dense(units[0], activation=activation, input_shape=(2,)))\n",
    "    for u in units[1:]:\n",
    "        model.add(Dense(u, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_plot(model, X, y, epochs=20, title=''):\n",
    "    h = model.fit(X, y, batch_size=32, epochs=epochs, validation_split=0.2, verbose=0)\n",
    "    # loss curves\n",
    "    fig, ax = plt.subplots(1,2, figsize=(9,3.8))\n",
    "    ax[0].plot(h.history['loss'], label='train')\n",
    "    ax[0].plot(h.history['val_loss'], label='val')\n",
    "    ax[0].set_title('Loss'); ax[0].legend()\n",
    "    ax[0].grid(True, alpha=0.3)\n",
    "    ax[1].plot(h.history['accuracy'], label='train')\n",
    "    ax[1].plot(h.history['val_accuracy'], label='val')\n",
    "    ax[1].set_title('Accuracy'); ax[1].legend()\n",
    "    ax[1].grid(True, alpha=0.3)\n",
    "    fig.suptitle(title)\n",
    "    plt.show()\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 — Layers and Units\n",
    "Try 1 hidden layer vs 2 hidden layers; vary hidden units (e.g., 4, 8, 16).\n",
    "- Record your validation accuracy and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = build_model(units=(8,), activation='relu', lr=1e-3)\n",
    "h1 = train_and_plot(m1, X, y, epochs=20, title='1 layer, 8 units, ReLU')\n",
    "\n",
    "m2 = build_model(units=(8,8), activation='relu', lr=1e-3)\n",
    "h2 = train_and_plot(m2, X, y, epochs=20, title='2 layers, 8-8 units, ReLU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 — Activation Choices\n",
    "Compare ReLU vs Tanh vs Sigmoid for hidden layers.\n",
    "- Which converges faster? Any saturation issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for act in ['relu','tanh','sigmoid']:\n",
    "    m = build_model(units=(16,16), activation=act, lr=1e-3)\n",
    "    _ = train_and_plot(m, X, y, epochs=25, title=f'2 layers, 16-16, activation={act}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 — Learning Rate Sensitivity\n",
    "Try a range of learning rates (e.g., 1e-4, 1e-3, 1e-2, 1e-1).\n",
    "- What happens for very large values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "    m = build_model(units=(8,8), activation='relu', lr=lr)\n",
    "    _ = train_and_plot(m, X, y, epochs=20, title=f'LR={lr}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional — Decision Regions\n",
    "Visualize learned decision boundaries for your favorite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regions(model, X, y):\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 220),\n",
    "        np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 220)\n",
    "    )\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()].astype('float32')\n",
    "    probs = model.predict(grid, verbose=0).reshape(xx.shape)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.contourf(xx, yy, probs, levels=20, cmap='RdBu', alpha=0.6)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, cmap='RdBu', edgecolor='k', s=10)\n",
    "    plt.title('Decision regions')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "m_best = build_model(units=(16,16), activation='relu', lr=1e-3)\n",
    "_ = m_best.fit(X, y, batch_size=32, epochs=25, validation_split=0.2, verbose=0)\n",
    "plot_regions(m_best, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Reflection\n",
    "Answer in your own words (2–4 sentences each):\n",
    "1. How does adding a second hidden layer change what the model can represent?\n",
    "2. When did you observe over- or under-fitting? What signaled it?\n",
    "3. Which activation worked best here, and why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d02fd0",
   "metadata": {},
   "source": [
    "## To-Do (stubs only — do not implement)\n",
    "These are optional extensions meant for practice. Leave them as TODOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a manual NumPy forward pass for a 2-layer network\n",
    "# (ReLU hidden, sigmoid output) on the same dataset.\n",
    "# Compare predictions qualitatively to the Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe3c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a function plot_regions_threshold(model, X, y, threshold=0.5)\n",
    "# that visualizes decision regions for different probability thresholds.\n",
    "# Try thresholds: 0.3, 0.5, 0.7 and note changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cbc764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Swap loss to mean-squared error (MSE) while keeping sigmoid output.\n",
    "# Train briefly and record any differences in convergence/accuracy vs BCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add L2 weight decay (kernel_regularizer) or a Dropout layer\n",
    "# and observe effects on training/validation curves.\n",
    "# Keep other hyperparameters the same for a fair comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
