{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 6 (Bonus) â€” Bag of Embeddings (BoE) Classifier\n",
        "\n",
        "In the previous notebook, we used a powerful LSTM model. However, sometimes a simple model is enough!\n",
        "\n",
        "In this notebook, we will build a **Bag of Embeddings** model. It works like this:\n",
        "1. **Lookup**: Get the word embedding for each word in the sentence.\n",
        "2. **Aggregate**: Sum (or average) all the embeddings into one vector representing the whole sentence.\n",
        "3. **Classify**: Pass that single vector through a linear layer to predict the class.\n",
        "\n",
        "We will still use **pretrained embeddings** (from Wikitext-103) to ensure the model knows what words mean before we even start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install fastai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastai.text.all import *\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data\n",
        "We use the same AG News dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj kyoto is xxmaj dead - xxmaj long xxmaj live xxmaj xxunk xxmaj there 's troubling news ( ft subscription xxunk , alternate copy here ) coming from xxmaj japan , where the xxmaj kyoto protocol on xxmaj greenhouse xxmaj emissions was born in 1997 . xxmaj it seems that the xxmaj japanese are n't going to be able to meet their emissions targets specified in the agreement in time . xxmaj indeed , unless they buy a \" large quantity \" of emissions credits from other countries , they 're not going to be able to meet their commitment at all . xxmaj xxunk xxmaj sugiyama , a climate expert at the xxmaj central xxmaj research xxmaj institute of xxmaj electric xxmaj power xxmaj industry in xxmaj japan , said emissions were rising 1 per cent a year due to a larger - than - expected impact from</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxbos xxmaj cape xxmaj clear boosts business processes in xxup esb xxmaj cape xxmaj clear xxmaj software this week is upgrading its xxup esb with the release of xxmaj cape xxmaj clear 6 , enabling development of business process xxunk based on xxunk &gt; advertisement &lt; / p&gt;&lt;p&gt;&lt;img src=\"http : / / ad.doubleclick.net / ad / idg.us.ifw.general / solaris;sz=1x1;ord=200301151450 ? \" width=\"1 \" height=\"1 \" border=\"0 \" / &gt; &lt; a href=\"http : / / ad.doubleclick.net / clk;12204780;10550054;n?http : / / ad.doubleclick.net / clk;12165994;105 xxrep 3 2 95;g?http : / / xxrep 3 w .sun.com / solaris10\"&gt;solaris 10(tm ) xxup os : xxmaj position your business ten moves ahead . &lt; / a&gt;&lt;br / &gt; solaris 10 xxup os has arrived and provides even more \\ reasons for the world 's most demanding businesses \\ to operate on this , the leading xxup unix platform . \\ xxmaj like the</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xxbos xxmaj wireless xxmaj san xxmaj francisco \\ \\ xxmaj reuters has a story about xxmaj gavin xxmaj newsom finally getting wifi and wanting to \\ hookup xxmaj san xxmaj francisco : \\ \" san xxup francisco ( reuters ) - xxmaj san xxmaj francisco xxmaj mayor xxmaj gavin xxmaj newsom has set a goal of \\ providing free wireless xxmaj internet activity in his city that sees itself as a \\ vanguard of the xxmaj internet revolution . \" \\ \" \" we will not stop until every xxmaj san xxmaj xxunk has access to free wireless \\ xxmaj internet service , \" he said in his annual state of the city address on \\ xxmaj thursday . \" these technologies will connect our residents to the skills and the \\ jobs of the new economy . \" \\ \\ xxmaj the issue i have is that xxmaj i</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "path = untar_data(URLs.AG_NEWS)\n",
        "df = pd.read_csv(path/'train.csv', header=None, names=['label', 'title', 'description'])\n",
        "df['text'] = df['title'] + \" \" + df['description']\n",
        "\n",
        "dls = TextDataLoaders.from_df(\n",
        "    df, \n",
        "    text_col='text', \n",
        "    label_col='label', \n",
        "    valid_pct=0.2, \n",
        "    bs=64\n",
        ")\n",
        "dls.show_batch(max_n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Get Pretrained Embeddings (Word2Vec / GloVe)\n",
        "\n",
        "Instead of using the AWD-LSTM embeddings, we can use standard **Word2Vec** or **GloVe** vectors. \n",
        "We will load them using `gensim`, create a matrix that maps our dataset's vocabulary to these pretrained vectors, and use that to initialize our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /Users/aghasi/repos/pa-academy-ai-2025/venv/lib/python3.13/site-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /Users/aghasi/repos/pa-academy-ai-2025/venv/lib/python3.13/site-packages (from gensim) (2.2.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /Users/aghasi/repos/pa-academy-ai-2025/venv/lib/python3.13/site-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /Users/aghasi/repos/pa-academy-ai-2025/venv/lib/python3.13/site-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /Users/aghasi/repos/pa-academy-ai-2025/venv/lib/python3.13/site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Loading pretrained vectors...\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Loaded!\n",
            "Loaded 34887 / 36760 words (94.9%) from pretrained embeddings.\n"
          ]
        }
      ],
      "source": [
        "# Install gensim if needed\n",
        "!pip install gensim\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load pretrained vectors (we use GloVe 100d as a lightweight proxy for Word2Vec)\n",
        "# The official Word2Vec is very large (1.5GB+), so we use a smaller high-quality one here.\n",
        "print(\"Loading pretrained vectors...\")\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\") \n",
        "print(\"Loaded!\")\n",
        "\n",
        "def create_emb_matrix(vocab, word_vectors, emb_dim):\n",
        "    vocab_size = len(vocab)\n",
        "    # Initialize with random weights (standard normal)\n",
        "    matrix = torch.randn((vocab_size, emb_dim))\n",
        "    \n",
        "    hits = 0\n",
        "    for i, word in enumerate(vocab):\n",
        "        try:\n",
        "            # Check if word exists in pretrained vectors\n",
        "            if word in word_vectors:\n",
        "                matrix[i] = torch.tensor(word_vectors[word])\n",
        "                hits += 1\n",
        "        except KeyError:\n",
        "            pass\n",
        "            \n",
        "    print(f\"Loaded {hits} / {vocab_size} words ({hits/vocab_size:.1%}) from pretrained embeddings.\")\n",
        "    return matrix\n",
        "\n",
        "# Create the matrix matching our dataset's vocabulary\n",
        "emb_dim = 100\n",
        "pretrained_weights = create_emb_matrix(dls.vocab[0], word_vectors, emb_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define the Bag of Embeddings Model\n",
        "\n",
        "We define a custom PyTorch module that sums the embeddings and applies a linear classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BoEWrapper(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, n_classes, pad_idx=1):\n",
        "        super().__init__()\n",
        "        # 1. Embedding Layer\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        # 2. Linear Layer\n",
        "        self.linear = nn.Linear(emb_dim, n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len)\n",
        "        with torch.no_grad():\n",
        "            embeddings = self.emb(x) # (batch, seq, emb_dim)\n",
        "        \n",
        "        # Mask padding (we don't want to sum the padding tokens)\n",
        "        # The padding token usually has index 1, but we look it up to be safe\n",
        "        mask = (x != self.emb.padding_idx).unsqueeze(-1) # (batch, seq, 1)\n",
        "        \n",
        "        # Zero out padding embeddings\n",
        "        embeddings = embeddings * mask.float()\n",
        "        \n",
        "        # 3. Mean along sequence dimension (dim=1)\n",
        "        summed = embeddings.mean(dim=1) # (batch, emb_dim)\n",
        "        \n",
        "        # 4. Linear layer -> logits\n",
        "        return self.linear(summed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized with Word2Vec/GloVe weights!\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "vocab_size = len(dls.vocab[0])\n",
        "emb_dim = 100                  # We used GloVe 100d\n",
        "n_classes = 4                  # AG News has 4 classes\n",
        "pad_idx = dls.vocab[0].index('xxpad') # Find the padding index\n",
        "\n",
        "# Initialize model\n",
        "boe_model = BoEWrapper(vocab_size, emb_dim, n_classes, pad_idx=pad_idx)\n",
        "\n",
        "# Load the pretrained weights we created\n",
        "boe_model.emb.weight.data.copy_(pretrained_weights)\n",
        "print(\"Model initialized with Word2Vec/GloVe weights!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train\n",
        "\n",
        "We wrap our custom model in a `Learner` and train. Since this is a simple linear layer on top of embeddings, it trains very fast!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.394418</td>\n",
              "      <td>0.410723</td>\n",
              "      <td>0.872000</td>\n",
              "      <td>00:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.366155</td>\n",
              "      <td>0.368930</td>\n",
              "      <td>0.879708</td>\n",
              "      <td>00:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.346865</td>\n",
              "      <td>0.359232</td>\n",
              "      <td>0.884708</td>\n",
              "      <td>00:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.350602</td>\n",
              "      <td>0.357008</td>\n",
              "      <td>0.884667</td>\n",
              "      <td>00:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.348423</td>\n",
              "      <td>0.356335</td>\n",
              "      <td>0.884500</td>\n",
              "      <td>00:29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create Learner\n",
        "simple_learn = Learner(dls, boe_model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "\n",
        "# Train\n",
        "simple_learn.fit_one_cycle(5, 5e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: 'Stock markets hit a new high today.'\n",
            "Topic: Business (0.99)\n",
            "Text: 'The goalkeeper saved the penalty kick.'\n",
            "Topic: Sports (1.00)\n",
            "Text: 'Astronomers discovered a new black hole.'\n",
            "Topic: Sci/Tech (1.00)\n"
          ]
        }
      ],
      "source": [
        "topics = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tech\"}\n",
        "\n",
        "def predict_simple(text):\n",
        "    # 1. Tokenize\n",
        "    tokenized = dls.tokenizer(text)\n",
        "    # 2. Map to IDs\n",
        "    ids = [dls.vocab[0].index(t) if t in dls.vocab[0] else 0 for t in tokenized]\n",
        "    # 3. Create tensor batch\n",
        "    tensor_in = torch.tensor(ids).unsqueeze(0).to(dls.device)\n",
        "    \n",
        "    # 4. Predict\n",
        "    with torch.no_grad():\n",
        "        logits = boe_model(tensor_in)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        pred_idx = probs.argmax().item()\n",
        "    \n",
        "    # Output\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"Topic: {topics[pred_idx+1]} ({probs.max():.2f})\")\n",
        "\n",
        "predict_simple(\"Stock markets hit a new high today.\")\n",
        "predict_simple(\"The goalkeeper saved the penalty kick.\")\n",
        "predict_simple(\"Astronomers discovered a new black hole.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
