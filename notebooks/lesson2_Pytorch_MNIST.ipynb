{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtKvmZx-WmUu"
      },
      "outputs": [],
      "source": [
        "#@title Installing PyTorch and Dependencies\n",
        "\n",
        "%pip install torch\n",
        "%pip install torchvision\n",
        "%pip install scikit-learn\n",
        "%pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGU6NwlsXFSt"
      },
      "outputs": [],
      "source": [
        "#@title Import Dependencies\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Enable interactive plotting\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bNfVLRUYqZA"
      },
      "outputs": [],
      "source": [
        "#@title Define Hyperparameters\n",
        "\n",
        "input_size = 784 # img_size = (28,28) ---> 28*28=784 in total\n",
        "hidden_size = 500 # number of nodes at hidden layer\n",
        "num_classes = 10 # number of output classes discrete range [0,9]\n",
        "num_epochs = 20 # number of times which the entire dataset is passed throughout the model\n",
        "batch_size = 100 # the size of input data took for one iteration\n",
        "lr = 1e-3 # size of step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCsBCXMwbpH5"
      },
      "outputs": [],
      "source": [
        "#@title Downloading MNIST data\n",
        "\n",
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfDPBdnYgfGp"
      },
      "outputs": [],
      "source": [
        "#@title Loading the data\n",
        "\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size,\n",
        "                                      shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL-YXTvghaz_"
      },
      "outputs": [],
      "source": [
        "#@title Define model class\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(Net,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3EPEqbjjfAT"
      },
      "outputs": [],
      "source": [
        "#@title Build the model\n",
        "\n",
        "net = Net(input_size, hidden_size, num_classes)\n",
        "net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePLIwvAFj2zH"
      },
      "outputs": [],
      "source": [
        "#@title Define loss-function & optimizer\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam( net.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u75Xa5VckuTH"
      },
      "outputs": [],
      "source": [
        "#@title Training the model with live visualization\n",
        "\n",
        "# Lists to store metrics\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "epochs_list = []\n",
        "\n",
        "# Function to evaluate on test set\n",
        "def evaluate_model():\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_gen:\n",
        "            images = Variable(images.view(-1, 28*28))\n",
        "            labels = Variable(labels)\n",
        "            \n",
        "            outputs = net(images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    avg_loss = total_loss / len(test_gen)\n",
        "    accuracy = 100 * correct / total\n",
        "    net.train()\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Function to plot metrics\n",
        "def plot_metrics():\n",
        "    clear_output(wait=True)\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
        "    \n",
        "    # Plot losses\n",
        "    ax1.plot(epochs_list, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
        "    ax1.plot(epochs_list, test_losses, 'r-', label='Test Loss', linewidth=2)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.set_title('Training and Test Loss', fontsize=14)\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot accuracy\n",
        "    ax2.plot(epochs_list, test_accuracies, 'g-', linewidth=2)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax2.set_title('Test Accuracy', fontsize=14)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim([0, 100])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    epoch_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for i, (images, labels) in enumerate(train_gen):\n",
        "        images = Variable(images.view(-1, 28*28))\n",
        "        labels = Variable(labels)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                  %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))\n",
        "    \n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = epoch_loss / num_batches\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    test_loss, test_accuracy = evaluate_model()\n",
        "    \n",
        "    # Store metrics\n",
        "    epochs_list.append(epoch + 1)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    \n",
        "    # Plot metrics\n",
        "    plot_metrics()\n",
        "    print(f'\\nEpoch [{epoch+1}/{num_epochs}] Summary:')\n",
        "    print(f'  Train Loss: {avg_train_loss:.4f}')\n",
        "    print(f'  Test Loss: {test_loss:.4f}')\n",
        "    print(f'  Test Accuracy: {test_accuracy:.2f}%\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTPvMW5jHB9X"
      },
      "outputs": [],
      "source": [
        "#@title Final Evaluation Summary\n",
        "\n",
        "print('='*60)\n",
        "print('FINAL MODEL PERFORMANCE')\n",
        "print('='*60)\n",
        "print(f'Best Test Accuracy: {max(test_accuracies):.2f}%')\n",
        "print(f'Final Test Accuracy: {test_accuracies[-1]:.2f}%')\n",
        "print(f'Final Train Loss: {train_losses[-1]:.4f}')\n",
        "print(f'Final Test Loss: {test_losses[-1]:.4f}')\n",
        "print('='*60)\n",
        "\n",
        "# Re-plot final metrics\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "# Plot losses\n",
        "ax1.plot(epochs_list, train_losses, 'b-', label='Train Loss', linewidth=2, marker='o')\n",
        "ax1.plot(epochs_list, test_losses, 'r-', label='Test Loss', linewidth=2, marker='s')\n",
        "ax1.set_xlabel('Epoch', fontsize=12)\n",
        "ax1.set_ylabel('Loss', fontsize=12)\n",
        "ax1.set_title('Training and Test Loss', fontsize=14)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot accuracy\n",
        "ax2.plot(epochs_list, test_accuracies, 'g-', linewidth=2, marker='o')\n",
        "ax2.set_xlabel('Epoch', fontsize=12)\n",
        "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax2.set_title('Test Accuracy', fontsize=14)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_ylim([0, 100])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Visualize Sample Predictions\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Get a batch of test images\n",
        "dataiter = iter(test_gen)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Make predictions\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    images_flat = Variable(images.view(-1, 28*28))\n",
        "    outputs = net(images_flat)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "# Convert to numpy for plotting\n",
        "images_np = images.numpy()\n",
        "predicted_np = predicted.numpy()\n",
        "labels_np = labels.numpy()\n",
        "\n",
        "# Plot 20 samples (4 rows x 5 columns)\n",
        "fig, axes = plt.subplots(4, 5, figsize=(12, 10))\n",
        "fig.suptitle('Sample Predictions (Green=Correct, Red=Wrong)', fontsize=16, y=0.995)\n",
        "\n",
        "for idx, ax in enumerate(axes.flat):\n",
        "    if idx < len(images_np):\n",
        "        # Display image\n",
        "        ax.imshow(images_np[idx].squeeze(), cmap='gray')\n",
        "        \n",
        "        # Check if prediction is correct\n",
        "        is_correct = predicted_np[idx] == labels_np[idx]\n",
        "        color = 'green' if is_correct else 'red'\n",
        "        \n",
        "        # Set title with prediction and actual label\n",
        "        title = f'Pred: {predicted_np[idx]}\\nActual: {labels_np[idx]}'\n",
        "        ax.set_title(title, fontsize=10, color=color, weight='bold')\n",
        "        ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics for this batch\n",
        "num_correct = (predicted_np == labels_np).sum()\n",
        "print(f'\\nBatch Statistics:')\n",
        "print(f'Correct predictions: {num_correct}/{len(labels_np)} ({100*num_correct/len(labels_np):.1f}%)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Find and Display Misclassified Examples\n",
        "\n",
        "# Collect misclassified examples\n",
        "net.eval()\n",
        "misclassified_images = []\n",
        "misclassified_labels = []\n",
        "misclassified_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_gen:\n",
        "        images_flat = Variable(images.view(-1, 28*28))\n",
        "        outputs = net(images_flat)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        # Find misclassified samples in this batch\n",
        "        wrong_idx = (predicted != labels).nonzero(as_tuple=True)[0]\n",
        "        \n",
        "        for idx in wrong_idx:\n",
        "            if len(misclassified_images) < 20:  # Collect up to 20 examples\n",
        "                misclassified_images.append(images[idx].numpy())\n",
        "                misclassified_labels.append(labels[idx].item())\n",
        "                misclassified_preds.append(predicted[idx].item())\n",
        "        \n",
        "        if len(misclassified_images) >= 20:\n",
        "            break\n",
        "\n",
        "# Plot misclassified examples\n",
        "if len(misclassified_images) > 0:\n",
        "    num_examples = min(20, len(misclassified_images))\n",
        "    rows = (num_examples + 4) // 5  # Calculate needed rows\n",
        "    \n",
        "    fig, axes = plt.subplots(rows, 5, figsize=(12, 2.5*rows))\n",
        "    fig.suptitle('Misclassified Examples', fontsize=16, color='red', y=0.995)\n",
        "    \n",
        "    # Flatten axes if needed\n",
        "    if rows == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for idx, ax in enumerate(axes.flat):\n",
        "        if idx < num_examples:\n",
        "            # Display image\n",
        "            ax.imshow(misclassified_images[idx].squeeze(), cmap='gray')\n",
        "            \n",
        "            # Set title with prediction and actual label\n",
        "            title = f'Pred: {misclassified_preds[idx]}\\nActual: {misclassified_labels[idx]}'\n",
        "            ax.set_title(title, fontsize=10, color='red', weight='bold')\n",
        "            ax.axis('off')\n",
        "        else:\n",
        "            ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f'\\nShowing {num_examples} misclassified examples')\n",
        "    print('These are cases where the model made mistakes.')\n",
        "    print('Analyzing these can help identify patterns in model errors.')\n",
        "else:\n",
        "    print('No misclassified examples found in the test set!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Confusion Matrix and Per-Class Accuracy\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Collect all predictions and labels\n",
        "net.eval()\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_gen:\n",
        "        images_flat = Variable(images.view(-1, 28*28))\n",
        "        outputs = net(images_flat)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        all_predictions.extend(predicted.numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "# Calculate per-class accuracy\n",
        "class_accuracies = []\n",
        "class_totals = []\n",
        "for digit in range(10):\n",
        "    digit_mask = all_labels == digit\n",
        "    digit_correct = (all_predictions[digit_mask] == digit).sum()\n",
        "    digit_total = digit_mask.sum()\n",
        "    class_totals.append(digit_total)\n",
        "    accuracy = 100 * digit_correct / digit_total if digit_total > 0 else 0\n",
        "    class_accuracies.append(accuracy)\n",
        "\n",
        "# Plot confusion matrix and per-class accuracy\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Confusion Matrix\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1, cbar_kws={'label': 'Count'})\n",
        "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
        "ax1.set_ylabel('True Label', fontsize=12)\n",
        "ax1.set_title('Confusion Matrix', fontsize=14)\n",
        "\n",
        "# Per-Class Accuracy\n",
        "digits = list(range(10))\n",
        "colors = ['green' if acc >= 98 else 'orange' if acc >= 95 else 'red' for acc in class_accuracies]\n",
        "bars = ax2.bar(digits, class_accuracies, color=colors, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, acc, total) in enumerate(zip(bars, class_accuracies, class_totals)):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "            f'{acc:.1f}%\\n({total})',\n",
        "            ha='center', va='bottom', fontsize=9, weight='bold')\n",
        "\n",
        "ax2.set_xlabel('Digit', fontsize=12)\n",
        "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax2.set_title('Per-Class Accuracy (with sample counts)', fontsize=14)\n",
        "ax2.set_ylim([0, 105])\n",
        "ax2.set_xticks(digits)\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor='green', label='â‰¥98% (Excellent)'),\n",
        "    Patch(facecolor='orange', label='95-98% (Good)'),\n",
        "    Patch(facecolor='red', label='<95% (Needs Improvement)')\n",
        "]\n",
        "ax2.legend(handles=legend_elements, loc='lower right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "print(f'\\n{\"=\"*60}')\n",
        "print('PER-CLASS PERFORMANCE')\n",
        "print(f'{\"=\"*60}')\n",
        "print(f'{\"Digit\":<8} {\"Accuracy\":<12} {\"Correct/Total\":<15} {\"Errors\"}')\n",
        "print(f'{\"-\"*60}')\n",
        "for digit in range(10):\n",
        "    digit_mask = all_labels == digit\n",
        "    digit_correct = (all_predictions[digit_mask] == digit).sum()\n",
        "    digit_total = digit_mask.sum()\n",
        "    digit_errors = digit_total - digit_correct\n",
        "    accuracy = 100 * digit_correct / digit_total if digit_total > 0 else 0\n",
        "    print(f'{digit:<8} {accuracy:>6.2f}%      {digit_correct:>4}/{digit_total:<8}    {digit_errors:>4}')\n",
        "\n",
        "print(f'\\n{\"=\"*60}')\n",
        "print(f'Overall Accuracy: {100 * (all_predictions == all_labels).sum() / len(all_labels):.2f}%')\n",
        "print(f'{\"=\"*60}')\n",
        "\n",
        "print(f'\\nMost Confused Pairs (where model makes mistakes):')\n",
        "# Find most confused pairs\n",
        "confused_pairs = []\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        if i != j and cm[i, j] > 0:\n",
        "            confused_pairs.append((i, j, cm[i, j]))\n",
        "\n",
        "confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "for i, (true_label, pred_label, count) in enumerate(confused_pairs[:5]):\n",
        "    print(f'{i+1}. Digit {true_label} misclassified as {pred_label}: {count} times')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pytorch MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
